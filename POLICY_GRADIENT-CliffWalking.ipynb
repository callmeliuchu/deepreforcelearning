{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle  # 用 pickle 替代 cPickle\n",
    "import gym\n",
    "np.bool8 = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023476ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581cb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CliffWalking-v0\")\n",
    "observation, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c6ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40b8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea4182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d3937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85039e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self,output_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(48,200)\n",
    "        self.linear2 = nn.Linear(200,output_dim,bias=False)\n",
    "        \n",
    "       # Xavier 初始化 + 偏置归零\n",
    "        nn.init.xavier_normal_(self.embeddings.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,idxs):\n",
    "        x = self.embeddings(idxs)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "#     def sample_action(self,x):\n",
    "# #         with torch.no_grad():\n",
    "#                 # 添加探索机制\n",
    "#         if np.random.random() < 0.8:\n",
    "#             action = np.random.randint(0, 4)\n",
    "#             probs = self.forward(x).squeeze()\n",
    "# #             print(probs.shape,action)\n",
    "#             return torch.LongTensor([action]), torch.log(probs[action]+1e-8).unsqueeze(dim=-1)\n",
    "        \n",
    "#         prob = self.forward(x)\n",
    "#         dist = Categorical(prob)\n",
    "#         action = dist.sample()\n",
    "#         log_prob = dist.log_prob(action)\n",
    "#         return action,log_prob\n",
    "\n",
    "    def sample_action(self, x):\n",
    "        with torch.no_grad():  # 在采样时不需要计算梯度\n",
    "            probs = self.forward(x).squeeze()\n",
    "            \n",
    "            # 添加探索机制\n",
    "            if np.random.random() < 0.5:\n",
    "                action = np.random.randint(0, 4)\n",
    "                return (torch.LongTensor([action]), \n",
    "                       torch.log(probs[action]))\n",
    "            \n",
    "            # 使用策略选择动作\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards):\n",
    "    ans = np.zeros_like(rewards)\n",
    "    adding = 0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        adding = adding * 0.99 + rewards[t]\n",
    "        ans[t] = adding\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44088fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63effb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import RMSprop\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b818012",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(self, x):\n",
    "    with torch.no_grad():  # 在采样时不需要计算梯度\n",
    "        probs = self.forward(x).squeeze()\n",
    "\n",
    "        # 添加探索机制\n",
    "        if np.random.random() < 0.0:\n",
    "            action = np.random.randint(0, 4)\n",
    "            return (torch.LongTensor([action]), \n",
    "                   torch.log(probs[action]))\n",
    "\n",
    "        # 使用策略选择动作\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "# 替换方法\n",
    "import types\n",
    "model.sample_action = types.MethodType(sample_action, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d5e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = CliffWalking2x3Env()\n",
    "observation, _ = env.reset()\n",
    "episode_number = 0\n",
    "prev_x = None\n",
    "xs = []\n",
    "logps = []\n",
    "hs = []\n",
    "actions = []\n",
    "ys = []\n",
    "rewards = []\n",
    "train_time = 0\n",
    "total_sum = 0\n",
    "max_times = 0\n",
    "while True:\n",
    "    prev_o = observation\n",
    "    idxs = torch.LongTensor([observation]) # shape (1,)\n",
    "    action,log_prob = model.sample_action(idxs) # shape 1,  1,\n",
    "    \n",
    "    action = action.item() ## 转成数\n",
    "    logps.append(log_prob)\n",
    "#     log_prob = log_prob.item()\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)  # 更新为新的返回值\n",
    "    \n",
    "        # 修改奖励结构\n",
    "    if reward == -100:  # 掉入悬崖\n",
    "        reward = -10  # 减小惩罚力度，避免过度规避风险\n",
    "    elif terminated and observation == 47:  # 到达目标状态（右下角）\n",
    "        reward = 10  # 给予明显的正向奖励\n",
    "#     if terminated and observation == 47:  # 到达目标状态（右下角）\n",
    "#         reward = 10  # 给予明显的正向奖励\n",
    "\n",
    "#     reward = reward_shaper(prev_o,observation,terminated)\n",
    "# #     print(action,reward)\n",
    "#     if terminated:\n",
    "#         reward = 1000\n",
    "    \n",
    "    rewards.append(reward*1.0)\n",
    "    xs.append(prev_o)\n",
    "    actions.append(action)\n",
    "    total_sum += reward\n",
    "    max_times += 1\n",
    "    \n",
    "\n",
    "    if terminated or truncated or max_times > 1000:\n",
    "#         print(len(actions),actions[:30])\n",
    "        episode_number += 1\n",
    "        xs = np.vstack(xs).ravel()\n",
    "        actions = np.vstack(actions)\n",
    "        rewards = np.vstack(rewards).ravel()\n",
    "#         print(rewards)\n",
    "        rewards = discount_rewards(rewards)\n",
    "        rewards -= rewards.mean()\n",
    "        rewards /= (rewards.std()+1e-8)\n",
    "\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        xs = torch.LongTensor(xs)\n",
    "        actions = torch.LongTensor(actions) # B,1\n",
    "        \n",
    "        ps = model(xs) ### B,4\n",
    "#         print('ps',ps)\n",
    "        # 防止概率为0或1导致log计算爆炸\n",
    "#         logps = torch.cat(logps)\n",
    "#         print(logps.shape,rewards.shape)\n",
    "        logps = torch.log(ps.gather(dim=-1,index=actions)+1e-8).squeeze() # B,1\n",
    "#         print('ps',ps.shape)\n",
    "#         print('action',actions.shape)\n",
    "#         print('rewards',rewards)\n",
    "#         print('logps',logps)\n",
    "        loss = -(logps * rewards).sum()\n",
    "        #### \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         print('rewards',rewards)\n",
    "#         print('logps',ps,logps)\n",
    "        if episode_number % 10 == 0:\n",
    "            print('total',loss,total_sum,len(logps))\n",
    "        \n",
    "        xs = []\n",
    "        rewards = []\n",
    "        logps = []\n",
    "        hs = []\n",
    "        actions = []\n",
    "        prev_x = None\n",
    "        total_sum = 0\n",
    "        max_times = 0\n",
    "        observation, _ = env.reset()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def visualize_agent(env, model, num_episodes=5):\n",
    "    \"\"\"\n",
    "    渲染显示智能体的行动\n",
    "    \"\"\"\n",
    "    env = gym.make('CliffWalking-v0', render_mode='human')  # 创建可视化环境\n",
    "#     env = gym.make('CliffWalking-v0')\n",
    "    for episode in range(num_episodes):\n",
    "        state_tuple = env.reset()\n",
    "        state = state_tuple[0] if isinstance(state_tuple, tuple) else state_tuple\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}\")\n",
    "        \n",
    "        while not done:\n",
    "            env.render()  # 渲染当前状态\n",
    "  \n",
    "            # 使用训练好的策略选择动作\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.LongTensor([state])\n",
    "                probs = model(state_tensor)\n",
    "                action = probs.argmax().item()  # 使用最可能的动作\n",
    "#                 print(action)\n",
    "            \n",
    "            # 执行动作\n",
    "            step_result = env.step(action)\n",
    "            if len(step_result) == 4:\n",
    "                next_state, reward, done, _ = step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = step_result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # 添加小延迟使动作更容易观察\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(f\"Episode finished after {steps} steps. Total reward: {total_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# 在主程序最后添加：\n",
    "if __name__ == \"__main__\":    \n",
    "    # 训练完成后显示智能体行动\n",
    "    print(\"\\nVisualizing trained agent behavior...\")\n",
    "    visualize_agent(env, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430ecf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0901e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
