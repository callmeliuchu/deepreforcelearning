{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "056340b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_\n",
    "import gym\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e694cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee621b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_states,n_actions,hidden_dim=128):\n",
    "        super(MLP,self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_states,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim,n_actions)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784c61af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ade45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = model.to('cpu')\n",
    "m2 = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee86389a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 is m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90c2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97a8ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self,capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "    \n",
    "    def push(self,transitions):\n",
    "        self.buffer.append(transitions)\n",
    "    \n",
    "    def sample(self):\n",
    "        return zip(*self.buffer)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf7b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d35eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \n",
    "    def __init__(self,model,memory,cfg):\n",
    "        \n",
    "        self.n_actions = cfg['n_actions']\n",
    "        self.device = torch.device(cfg['device'])\n",
    "        self.gamma = cfg['gamma']\n",
    "        self.sample_count = 0\n",
    "        self.epsilon = cfg['epsilon_start']\n",
    "        self.epsilon_start = cfg['epsilon_start']\n",
    "        self.epsilon_end = cfg['epsilon_end']\n",
    "        self.epsilon_decay = cfg['epsilon_decay']\n",
    "        self.batch_size = cfg['batch_size']\n",
    "        \n",
    "        self.policy_net = model.to(self.device)\n",
    "        self.target_net = model.to(self.device)\n",
    "        \n",
    "        for target_param, param in zip(self.target_net.parameters(),\n",
    "                                      self.policy_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(),\n",
    "                                   lr=cfg['lr'])\n",
    "        self.memory = memory\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        ''' 采样动作\n",
    "        '''\n",
    "        self.sample_count += 1\n",
    "        # epsilon指数衰减\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.sample_count / self.epsilon_decay) \n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "                q_values = self.policy_net(state)\n",
    "                action = q_values.max(1)[1].item() # choose action corresponding to the maximum q value\n",
    "        else:\n",
    "            action = random.randrange(self.n_actions)\n",
    "        return action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_action(self,state):\n",
    "        state = torch.tensor(state, device=self.device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "        q_values = self.policy_net(state)\n",
    "        action = q_values.max(1)[1].item()\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < self.batch_size: # 当经验回放中不满足一个批量时，不更新策略\n",
    "            return\n",
    "        # 从经验回放中随机采样一个批量的转移(transition)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.memory.sample(\n",
    "           )\n",
    "        # 将数据转换为tensor\n",
    "        state_batch = torch.tensor(np.array(state_batch), device=self.device, dtype=torch.float)\n",
    "        action_batch = torch.tensor(action_batch, device=self.device).unsqueeze(1)  \n",
    "        reward_batch = torch.tensor(reward_batch, device=self.device, dtype=torch.float)  \n",
    "        next_state_batch = torch.tensor(np.array(next_state_batch), device=self.device, dtype=torch.float)\n",
    "        done_batch = torch.tensor(np.float32(done_batch), device=self.device)\n",
    "        \n",
    "        q_values = self.policy_net(state_batch).gather(dim=1,index=action_batch)\n",
    "        next_q_values = self.target_net(next_state_batch).max(1)[0].detach() # 计算下一时刻的状态(s_t_,a)对应的Q值\n",
    "        \n",
    "        expected_q_values = reward_batch + self.gamma * next_q_values * (1-done_batch)\n",
    "        \n",
    "        loss = nn.MSELoss()(q_values,expected_q_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1,1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9136c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    ''' 训练\n",
    "    '''\n",
    "    print(\"开始训练！\")\n",
    "    rewards = []  # 记录所有回合的奖励\n",
    "    steps = []\n",
    "    for i_ep in range(cfg['train_eps']):\n",
    "        ep_reward = 0  # 记录一回合内的奖励\n",
    "        ep_step = 0\n",
    "        state,_ = env.reset()  # 重置环境，返回初始状态\n",
    "        for _ in range(cfg['ep_max_steps']):\n",
    "            ep_step += 1\n",
    "            action = agent.sample_action(state)  # 选择动作\n",
    "#             next_state, reward, terminated, truncated, _\n",
    "            next_state, reward, done, _,_ = env.step(action)  # 更新环境，返回transition\n",
    "            agent.memory.push((state, action, reward,next_state, done))  # 保存transition\n",
    "            state = next_state  # 更新下一个状态\n",
    "            agent.update()  # 更新智能体\n",
    "            ep_reward += reward  # 累加奖励\n",
    "            if done:\n",
    "                break\n",
    "        if (i_ep + 1) % cfg['target_update'] == 0:  # 智能体目标网络更新\n",
    "            agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "        if (i_ep + 1) % 10 == 0:\n",
    "            print(f\"回合：{i_ep+1}/{cfg['train_eps']}，奖励：{ep_reward:.2f}，Epislon：{agent.epsilon:.3f}\")\n",
    "    print(\"完成训练！\")\n",
    "    env.close()\n",
    "    return {'rewards':rewards}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab4eec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg):\n",
    "    env = gym.make(cfg['env_name']) # 创建环境\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    cfg.update({\"n_states\":n_states,\"n_actions\":n_actions}) # 更新n_states和n_actions到cfg参数中\n",
    "    model = MLP(n_states, n_actions, hidden_dim = cfg['hidden_dim']) # 创建模型\n",
    "    memory = ReplayBuffer(cfg['memory_capacity'])\n",
    "    agent = DQN(model,memory,cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbf547d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def get_args():\n",
    "    \"\"\" 超参数\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"hyperparameters\")      \n",
    "    parser.add_argument('--algo_name',default='DQN',type=str,help=\"name of algorithm\")\n",
    "    parser.add_argument('--env_name',default='CartPole-v0',type=str,help=\"name of environment\")\n",
    "    parser.add_argument('--train_eps',default=200,type=int,help=\"episodes of training\")\n",
    "    parser.add_argument('--test_eps',default=20,type=int,help=\"episodes of testing\")\n",
    "    parser.add_argument('--ep_max_steps',default = 100000,type=int,help=\"steps per episode, much larger value can simulate infinite steps\")\n",
    "    parser.add_argument('--gamma',default=0.95,type=float,help=\"discounted factor\")\n",
    "    parser.add_argument('--epsilon_start',default=0.95,type=float,help=\"initial value of epsilon\")\n",
    "    parser.add_argument('--epsilon_end',default=0.01,type=float,help=\"final value of epsilon\")\n",
    "    parser.add_argument('--epsilon_decay',default=500,type=int,help=\"decay rate of epsilon, the higher value, the slower decay\")\n",
    "    parser.add_argument('--lr',default=0.0001,type=float,help=\"learning rate\")\n",
    "    parser.add_argument('--memory_capacity',default=100000,type=int,help=\"memory capacity\")\n",
    "    parser.add_argument('--batch_size',default=64,type=int)\n",
    "    parser.add_argument('--target_update',default=4,type=int)\n",
    "    parser.add_argument('--hidden_dim',default=256,type=int)\n",
    "    parser.add_argument('--device',default='cpu',type=str,help=\"cpu or cuda\") \n",
    "    parser.add_argument('--seed',default=10,type=int,help=\"seed\")   \n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))      \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa53fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t        DQN         \t   <class 'str'>    \n",
      "      env_name      \t    CartPole-v0     \t   <class 'str'>    \n",
      "     train_eps      \t        200         \t   <class 'int'>    \n",
      "      test_eps      \t         20         \t   <class 'int'>    \n",
      "    ep_max_steps    \t       100000       \t   <class 'int'>    \n",
      "       gamma        \t        0.95        \t  <class 'float'>   \n",
      "   epsilon_start    \t        0.95        \t  <class 'float'>   \n",
      "    epsilon_end     \t        0.01        \t  <class 'float'>   \n",
      "   epsilon_decay    \t        500         \t   <class 'int'>    \n",
      "         lr         \t       0.0001       \t  <class 'float'>   \n",
      "  memory_capacity   \t       100000       \t   <class 'int'>    \n",
      "     batch_size     \t         64         \t   <class 'int'>    \n",
      "   target_update    \t         4          \t   <class 'int'>    \n",
      "     hidden_dim     \t        256         \t   <class 'int'>    \n",
      "       device       \t        cpu         \t   <class 'str'>    \n",
      "        seed        \t         10         \t   <class 'int'>    \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg = get_args() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "256cb867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2\n",
      "开始训练！\n",
      "回合：10/200，奖励：14.00，Epislon：0.612\n",
      "回合：20/200，奖励：10.00，Epislon：0.464\n",
      "回合：30/200，奖励：14.00，Epislon：0.368\n",
      "回合：40/200，奖励：64.00，Epislon：0.198\n",
      "回合：50/200，奖励：759.00，Epislon：0.021\n",
      "回合：60/200，奖励：232.00，Epislon：0.010\n",
      "回合：70/200，奖励：148.00，Epislon：0.010\n",
      "回合：80/200，奖励：107.00，Epislon：0.010\n",
      "回合：90/200，奖励：164.00，Epislon：0.010\n",
      "回合：100/200，奖励：283.00，Epislon：0.010\n",
      "回合：110/200，奖励：301.00，Epislon：0.010\n",
      "回合：120/200，奖励：204.00，Epislon：0.010\n",
      "回合：130/200，奖励：211.00，Epislon：0.010\n",
      "回合：140/200，奖励：234.00，Epislon：0.010\n",
      "回合：150/200，奖励：225.00，Epislon：0.010\n",
      "回合：160/200，奖励：169.00，Epislon：0.010\n",
      "回合：170/200，奖励：212.00，Epislon：0.010\n",
      "回合：180/200，奖励：266.00，Epislon：0.010\n",
      "回合：190/200，奖励：115.00，Epislon：0.010\n",
      "回合：200/200，奖励：165.00，Epislon：0.010\n",
      "完成训练！\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "res_dic = train(cfg, env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09118d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e48a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1e5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e9be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
