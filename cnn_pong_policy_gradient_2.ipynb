{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18f391aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61504f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# 创建环境\n",
    "env = gym.make(\"Pong-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "299a2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# state,_ = env.reset()\n",
    "# done = False\n",
    "# count = 0\n",
    "# while not done:\n",
    "# #     env.render()\n",
    "#     action = int(np.random.choice([2,3]))\n",
    "#     next_state, reward, done, truncated, _ = env.step(action)\n",
    "#     print(action,reward)\n",
    "#     count += 1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5bc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5e00ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f5b4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\"将 210x160x3 uint8 帧预处理为 6400 (80x80) 1D float 向量\"\"\"\n",
    "    I = I[35:195]  # 裁剪\n",
    "    I = I[::2, ::2, 0]  # 下采样因子为 2\n",
    "    I[I == 144] = 0  # 删除背景类型 1\n",
    "    I[I == 109] = 0  # 删除背景类型 2\n",
    "    I[I != 0] = 1  # 其他设置为 1\n",
    "    return I.astype(np.float32).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e04dd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageCNN(nn.Module):\n",
    "    def __init__(self, output_dim: int = 10):\n",
    "        super(ImageCNN, self).__init__()\n",
    "        \n",
    "        # 特征提取部分（总参数：234）\n",
    "        self.features = nn.Sequential(\n",
    "            # 卷积层1：kernel_size=2 减少参数\n",
    "            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=8),  # 输出形状: (12, 209, 159)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=8)  # 输出形状: (12, 104, 79)\n",
    "        )\n",
    "        \n",
    "        # 全局平均池化 + 分类器（总参数：12*output_dim + output_dim）\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),  # 输出形状: (12, 1, 1)\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(12, output_dim)       # 输出形状: (batch_size, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 输入归一化\n",
    "        x = x.float() / 255.0\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a80a79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state,_ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a5ee4c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9e00601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "064e8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def numpy_to_tensor(numpy_data: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    将形状为 (210, 160, 3) 的NumPy数组转换为PyTorch模型所需的Tensor格式\n",
    "    步骤：\n",
    "      1. 归一化到 [0,1]\n",
    "      2. 转换通道顺序 HWC -> CHW\n",
    "      3. 添加batch维度\n",
    "      4. 转为PyTorch Tensor\n",
    "    \"\"\"\n",
    "    # 验证输入形状\n",
    "    assert numpy_data.shape == (210, 160, 3), \\\n",
    "        f\"输入形状应为 (210, 160, 3)，但得到 {numpy_data.shape}\"\n",
    "    \n",
    "    # 1. 转换为float32并归一化\n",
    "    tensor_data = numpy_data.astype(np.float32) / 255.0\n",
    "    \n",
    "    # 2. 调整通道顺序 (HWC -> CHW)\n",
    "    tensor_data = np.transpose(tensor_data, (2, 0, 1))  # 输出形状: (3, 210, 160)\n",
    "    \n",
    "    # 3. 添加batch维度 (CHW -> BCHW)\n",
    "    tensor_data = np.expand_dims(tensor_data, axis=0)  # 输出形状: (1, 3, 210, 160)\n",
    "    \n",
    "    # 4. 转为PyTorch Tensor\n",
    "    return torch.from_numpy(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e144d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "149824bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = numpy_to_tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "497d7af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 210, 160])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7eb3adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bb89401d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 12x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[145], line 27\u001b[0m, in \u001b[0;36mImageCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 12x10)"
     ]
    }
   ],
   "source": [
    "model(tt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638587ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904f428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5024f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.project = ImageCNN(100)\n",
    "        self.linear1 = nn.Linear(input_dim,200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(200,output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,image_data):\n",
    "        ### n\n",
    "        state = self.project(image_data)\n",
    "        state = state.squeeze(0)\n",
    "#         print(state.shape)\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x) # n\n",
    "        x = self.softmax(x) # n\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4d628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "52feccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_\n",
    "\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cada96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.policy_net = PolicyNet(100,2)\n",
    "        self.optimizer = AdamW(self.policy_net.parameters(),lr=1e-3)\n",
    "    \n",
    "    def sample_action(self,state):\n",
    "        probs = self.policy_net(state) # 4\n",
    "        if np.random.uniform() < 0.2:\n",
    "            action = np.random.randint(0,2)\n",
    "            return action + 2, torch.log(probs[action]+1e-8)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item()+2,log_prob\n",
    "    \n",
    "    def update(self,rewards,log_probs):\n",
    "        ### 一次游戏时间\n",
    "        ret = []\n",
    "        adding = 0\n",
    "        for r in rewards[::-1]:\n",
    "            if r != 0:\n",
    "                adding = 0\n",
    "            adding = adding * 0.99 + r\n",
    "            ret.insert(0,adding)\n",
    "        ret = torch.FloatTensor(ret)\n",
    "        ret = ret - ret.mean()\n",
    "        ret = ret / (ret.std()+1e-8)\n",
    "        \n",
    "        r_log_probs = []\n",
    "        for r,log_prob in zip(ret,log_probs):\n",
    "            r_log_probs.append(-r*log_prob)\n",
    "        r_log_probs = torch.vstack(r_log_probs)\n",
    "        \n",
    "        loss = r_log_probs.sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184d817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "36c762a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env):\n",
    "    success_count = []\n",
    "    max_size = 2000\n",
    "    for epoch in range(20000):\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        terminated = False\n",
    "        state,_ = env.reset()\n",
    "        prev_x = None\n",
    "        while not terminated:\n",
    "            diff = numpy_to_tensor(state)\n",
    "            action, log_prob = agent.sample_action(diff)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "        \n",
    "        loss = agent.update(rewards,log_probs) \n",
    "        \n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "#             torch.save('pong.pt',agent.policy_net)\n",
    "            torch.save(agent.policy_net,'pong.pt')\n",
    "            print(f'epoch: {epoch}, loss: {loss}, rewards: {sum(rewards)}, count: {len(rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "57d0b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0b264415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.policy_net,'pong.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8e722c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load('pong.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7bd2bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: -1.5405373573303223, rewards: -21.0, count: 1264\n",
      "epoch: 19, loss: 9.316274642944336, rewards: -20.0, count: 1303\n",
      "epoch: 29, loss: -2.579474925994873, rewards: -21.0, count: 1452\n",
      "epoch: 39, loss: 0.5740667581558228, rewards: -20.0, count: 1223\n",
      "epoch: 49, loss: 0.934499204158783, rewards: -21.0, count: 1255\n",
      "epoch: 59, loss: 1.7972705364227295, rewards: -20.0, count: 1536\n",
      "epoch: 69, loss: -5.789192199707031, rewards: -21.0, count: 1437\n",
      "epoch: 79, loss: -2.7945001125335693, rewards: -21.0, count: 1280\n",
      "epoch: 89, loss: -4.030975818634033, rewards: -21.0, count: 1176\n",
      "epoch: 99, loss: 3.4906625747680664, rewards: -21.0, count: 1500\n",
      "epoch: 109, loss: 0.192344069480896, rewards: -21.0, count: 1181\n",
      "epoch: 119, loss: 1.6203093528747559, rewards: -20.0, count: 1385\n",
      "epoch: 129, loss: -6.714287757873535, rewards: -21.0, count: 1105\n",
      "epoch: 139, loss: 1.8146634101867676, rewards: -21.0, count: 1427\n",
      "epoch: 149, loss: 3.6673386096954346, rewards: -19.0, count: 1517\n",
      "epoch: 159, loss: -3.084707736968994, rewards: -21.0, count: 1595\n",
      "epoch: 169, loss: 3.2200090885162354, rewards: -20.0, count: 1390\n",
      "epoch: 179, loss: -5.709425926208496, rewards: -21.0, count: 1429\n",
      "epoch: 189, loss: 5.769996166229248, rewards: -21.0, count: 1253\n",
      "epoch: 199, loss: 1.8854949474334717, rewards: -20.0, count: 1232\n",
      "epoch: 209, loss: -0.3408409357070923, rewards: -21.0, count: 1187\n",
      "epoch: 219, loss: -4.084537982940674, rewards: -21.0, count: 1270\n",
      "epoch: 229, loss: -5.694547176361084, rewards: -21.0, count: 1251\n",
      "epoch: 239, loss: -7.3446946144104, rewards: -21.0, count: 1506\n",
      "epoch: 249, loss: -3.0292704105377197, rewards: -21.0, count: 1346\n",
      "epoch: 259, loss: -4.530816555023193, rewards: -21.0, count: 1245\n",
      "epoch: 269, loss: -3.883178472518921, rewards: -21.0, count: 1099\n",
      "epoch: 279, loss: 2.1764140129089355, rewards: -21.0, count: 1255\n",
      "epoch: 289, loss: 0.4646601676940918, rewards: -19.0, count: 1451\n",
      "epoch: 299, loss: -5.055375099182129, rewards: -21.0, count: 1276\n",
      "epoch: 309, loss: -6.570610523223877, rewards: -21.0, count: 1350\n",
      "epoch: 319, loss: -1.6054065227508545, rewards: -20.0, count: 1233\n",
      "epoch: 329, loss: 0.5444788932800293, rewards: -21.0, count: 1345\n",
      "epoch: 339, loss: -4.428532600402832, rewards: -21.0, count: 1254\n",
      "epoch: 349, loss: 5.681793689727783, rewards: -19.0, count: 1507\n",
      "epoch: 359, loss: 1.2630653381347656, rewards: -21.0, count: 1174\n",
      "epoch: 369, loss: 1.2323410511016846, rewards: -21.0, count: 1163\n",
      "epoch: 379, loss: 2.585221767425537, rewards: -21.0, count: 1123\n",
      "epoch: 389, loss: 6.970017433166504, rewards: -19.0, count: 1768\n",
      "epoch: 399, loss: -0.9805252552032471, rewards: -20.0, count: 1226\n",
      "epoch: 409, loss: 4.780807018280029, rewards: -19.0, count: 1691\n",
      "epoch: 419, loss: -0.7345684766769409, rewards: -20.0, count: 1302\n",
      "epoch: 429, loss: -1.9260191917419434, rewards: -20.0, count: 1491\n",
      "epoch: 439, loss: 2.299529552459717, rewards: -19.0, count: 1380\n",
      "epoch: 449, loss: 1.792252540588379, rewards: -19.0, count: 1676\n",
      "epoch: 459, loss: 1.8387293815612793, rewards: -21.0, count: 1185\n",
      "epoch: 469, loss: 1.1199800968170166, rewards: -20.0, count: 1221\n",
      "epoch: 479, loss: 1.7203296422958374, rewards: -20.0, count: 1633\n",
      "epoch: 489, loss: -2.8567609786987305, rewards: -21.0, count: 1104\n",
      "epoch: 499, loss: 4.587249279022217, rewards: -20.0, count: 1306\n",
      "epoch: 509, loss: -0.7933206558227539, rewards: -20.0, count: 1547\n",
      "epoch: 519, loss: 0.964627742767334, rewards: -21.0, count: 1336\n",
      "epoch: 529, loss: -2.7298874855041504, rewards: -21.0, count: 1260\n",
      "epoch: 539, loss: 0.42391443252563477, rewards: -20.0, count: 1240\n",
      "epoch: 549, loss: 3.234421730041504, rewards: -21.0, count: 1179\n",
      "epoch: 559, loss: -1.8278086185455322, rewards: -20.0, count: 1396\n",
      "epoch: 569, loss: -4.924278736114502, rewards: -21.0, count: 1435\n",
      "epoch: 579, loss: 2.558272361755371, rewards: -21.0, count: 1516\n",
      "epoch: 589, loss: 3.1099157333374023, rewards: -21.0, count: 1588\n",
      "epoch: 599, loss: 0.5355398058891296, rewards: -21.0, count: 1100\n",
      "epoch: 609, loss: -1.2081059217453003, rewards: -21.0, count: 1088\n",
      "epoch: 619, loss: 3.3020622730255127, rewards: -20.0, count: 1375\n",
      "epoch: 629, loss: 1.4062602519989014, rewards: -20.0, count: 1537\n",
      "epoch: 639, loss: 5.4370527267456055, rewards: -21.0, count: 1110\n",
      "epoch: 649, loss: -4.377676486968994, rewards: -21.0, count: 1420\n",
      "epoch: 659, loss: -2.9361281394958496, rewards: -21.0, count: 1104\n",
      "epoch: 669, loss: 6.287530899047852, rewards: -20.0, count: 1471\n",
      "epoch: 679, loss: 9.162004470825195, rewards: -19.0, count: 1519\n",
      "epoch: 689, loss: 9.287734985351562, rewards: -19.0, count: 1444\n",
      "epoch: 699, loss: -3.9830269813537598, rewards: -20.0, count: 1542\n",
      "epoch: 709, loss: -0.2526165246963501, rewards: -21.0, count: 1410\n",
      "epoch: 719, loss: -0.45461201667785645, rewards: -21.0, count: 1264\n",
      "epoch: 729, loss: 1.915614366531372, rewards: -20.0, count: 1640\n",
      "epoch: 739, loss: 1.509055733680725, rewards: -19.0, count: 1359\n",
      "epoch: 749, loss: 1.2572300434112549, rewards: -20.0, count: 1935\n",
      "epoch: 759, loss: 3.4714736938476562, rewards: -20.0, count: 1613\n",
      "epoch: 769, loss: -3.917184829711914, rewards: -21.0, count: 1354\n",
      "epoch: 779, loss: -1.4771909713745117, rewards: -21.0, count: 1323\n",
      "epoch: 789, loss: 0.48845934867858887, rewards: -20.0, count: 1400\n",
      "epoch: 799, loss: 1.4889531135559082, rewards: -21.0, count: 1092\n",
      "epoch: 809, loss: -2.1998302936553955, rewards: -20.0, count: 1477\n",
      "epoch: 819, loss: 0.5502490997314453, rewards: -21.0, count: 1574\n",
      "epoch: 829, loss: -1.1722419261932373, rewards: -20.0, count: 1631\n",
      "epoch: 839, loss: 2.1614108085632324, rewards: -20.0, count: 1475\n",
      "epoch: 849, loss: -2.0421621799468994, rewards: -21.0, count: 1261\n",
      "epoch: 859, loss: 1.3236260414123535, rewards: -20.0, count: 1446\n",
      "epoch: 869, loss: -5.20963191986084, rewards: -21.0, count: 1248\n",
      "epoch: 879, loss: -10.346978187561035, rewards: -21.0, count: 1254\n",
      "epoch: 889, loss: -0.9723672866821289, rewards: -21.0, count: 1505\n",
      "epoch: 899, loss: -4.336328029632568, rewards: -21.0, count: 1180\n",
      "epoch: 909, loss: 1.2389583587646484, rewards: -21.0, count: 1420\n",
      "epoch: 919, loss: -3.4861631393432617, rewards: -21.0, count: 1834\n",
      "epoch: 929, loss: 12.224103927612305, rewards: -21.0, count: 1263\n",
      "epoch: 939, loss: -5.805069923400879, rewards: -20.0, count: 1451\n",
      "epoch: 949, loss: -14.04869270324707, rewards: -21.0, count: 1087\n",
      "epoch: 959, loss: 31.0956974029541, rewards: -20.0, count: 1294\n",
      "epoch: 969, loss: 10.955656051635742, rewards: -20.0, count: 1234\n",
      "epoch: 979, loss: 1.7502415180206299, rewards: -21.0, count: 1180\n",
      "epoch: 989, loss: 5.793882846832275, rewards: -21.0, count: 1425\n",
      "epoch: 999, loss: 1.6758670806884766, rewards: -21.0, count: 1205\n",
      "epoch: 1009, loss: -2.0400843620300293, rewards: -21.0, count: 1338\n",
      "epoch: 1019, loss: -3.523784637451172, rewards: -21.0, count: 1350\n",
      "epoch: 1029, loss: 0.5565128326416016, rewards: -20.0, count: 1701\n",
      "epoch: 1039, loss: 1.2874541282653809, rewards: -20.0, count: 1455\n",
      "epoch: 1049, loss: -1.7538774013519287, rewards: -21.0, count: 1102\n",
      "epoch: 1059, loss: 6.742029666900635, rewards: -20.0, count: 1304\n",
      "epoch: 1069, loss: -4.962184429168701, rewards: -20.0, count: 1631\n",
      "epoch: 1079, loss: 8.338409423828125, rewards: -20.0, count: 1390\n",
      "epoch: 1089, loss: -2.303455352783203, rewards: -20.0, count: 1229\n",
      "epoch: 1099, loss: -3.4782257080078125, rewards: -21.0, count: 1208\n",
      "epoch: 1109, loss: -6.157438278198242, rewards: -21.0, count: 1346\n",
      "epoch: 1119, loss: -0.09686243534088135, rewards: -20.0, count: 1717\n",
      "epoch: 1129, loss: 6.155501842498779, rewards: -21.0, count: 1266\n",
      "epoch: 1139, loss: -4.249179840087891, rewards: -21.0, count: 1089\n",
      "epoch: 1149, loss: 13.729981422424316, rewards: -20.0, count: 1480\n",
      "epoch: 1159, loss: 0.16105973720550537, rewards: -21.0, count: 1275\n",
      "epoch: 1169, loss: -1.3455877304077148, rewards: -21.0, count: 1399\n",
      "epoch: 1179, loss: 0.4734921455383301, rewards: -19.0, count: 1353\n",
      "epoch: 1189, loss: 4.222918510437012, rewards: -21.0, count: 1266\n",
      "epoch: 1199, loss: -0.5568017959594727, rewards: -21.0, count: 1577\n",
      "epoch: 1209, loss: -2.209676742553711, rewards: -19.0, count: 1595\n",
      "epoch: 1219, loss: 1.604550838470459, rewards: -20.0, count: 1240\n",
      "epoch: 1229, loss: 6.5260491371154785, rewards: -21.0, count: 1188\n",
      "epoch: 1239, loss: 11.279531478881836, rewards: -21.0, count: 1338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1249, loss: 0.1621636152267456, rewards: -21.0, count: 1154\n",
      "epoch: 1259, loss: 1.0230016708374023, rewards: -20.0, count: 1391\n",
      "epoch: 1269, loss: 3.307645082473755, rewards: -20.0, count: 1385\n",
      "epoch: 1279, loss: 1.2871754169464111, rewards: -21.0, count: 1120\n",
      "epoch: 1289, loss: -0.08570241928100586, rewards: -20.0, count: 1228\n",
      "epoch: 1299, loss: 4.659457206726074, rewards: -21.0, count: 1268\n",
      "epoch: 1309, loss: 6.132356643676758, rewards: -21.0, count: 1103\n",
      "epoch: 1319, loss: -5.305800914764404, rewards: -21.0, count: 1181\n",
      "epoch: 1329, loss: 3.8089215755462646, rewards: -21.0, count: 1352\n",
      "epoch: 1339, loss: 7.208146095275879, rewards: -21.0, count: 1094\n",
      "epoch: 1349, loss: 6.443857669830322, rewards: -20.0, count: 1469\n",
      "epoch: 1359, loss: 0.12514591217041016, rewards: -21.0, count: 1423\n",
      "epoch: 1369, loss: -7.249266624450684, rewards: -21.0, count: 1344\n",
      "epoch: 1379, loss: 2.024204969406128, rewards: -21.0, count: 1406\n",
      "epoch: 1389, loss: -1.8568624258041382, rewards: -21.0, count: 1182\n",
      "epoch: 1399, loss: 0.5079116821289062, rewards: -21.0, count: 1276\n",
      "epoch: 1409, loss: 4.207756042480469, rewards: -20.0, count: 1297\n",
      "epoch: 1419, loss: -0.5240234136581421, rewards: -21.0, count: 1335\n",
      "epoch: 1429, loss: -0.1984487771987915, rewards: -21.0, count: 1331\n",
      "epoch: 1439, loss: -0.09547567367553711, rewards: -20.0, count: 1393\n",
      "epoch: 1449, loss: 3.9019696712493896, rewards: -21.0, count: 1506\n",
      "epoch: 1459, loss: -1.1228971481323242, rewards: -21.0, count: 1089\n",
      "epoch: 1469, loss: -2.1800472736358643, rewards: -21.0, count: 1102\n",
      "epoch: 1479, loss: 2.20817232131958, rewards: -21.0, count: 1416\n",
      "epoch: 1489, loss: -0.3984835147857666, rewards: -21.0, count: 1511\n",
      "epoch: 1499, loss: -5.671444892883301, rewards: -19.0, count: 1764\n",
      "epoch: 1509, loss: 3.413322687149048, rewards: -20.0, count: 1251\n",
      "epoch: 1519, loss: -2.380397081375122, rewards: -21.0, count: 1187\n",
      "epoch: 1529, loss: 2.630770683288574, rewards: -21.0, count: 1249\n",
      "epoch: 1539, loss: -0.3640899658203125, rewards: -21.0, count: 1180\n",
      "epoch: 1549, loss: -1.549881935119629, rewards: -21.0, count: 1182\n",
      "epoch: 1559, loss: -1.4446169137954712, rewards: -21.0, count: 1164\n",
      "epoch: 1569, loss: 4.19144868850708, rewards: -21.0, count: 1103\n",
      "epoch: 1579, loss: 0.4387655258178711, rewards: -21.0, count: 1202\n",
      "epoch: 1589, loss: 5.214944839477539, rewards: -19.0, count: 1525\n",
      "epoch: 1599, loss: 0.24989724159240723, rewards: -21.0, count: 1414\n",
      "epoch: 1609, loss: -6.621613502502441, rewards: -21.0, count: 1432\n",
      "epoch: 1619, loss: 3.419057607650757, rewards: -21.0, count: 1198\n",
      "epoch: 1629, loss: 1.3179776668548584, rewards: -19.0, count: 1764\n",
      "epoch: 1639, loss: -0.47703802585601807, rewards: -21.0, count: 1102\n",
      "epoch: 1649, loss: -0.892859697341919, rewards: -21.0, count: 1032\n",
      "epoch: 1659, loss: -4.411725044250488, rewards: -21.0, count: 1495\n",
      "epoch: 1669, loss: 1.882108211517334, rewards: -20.0, count: 1390\n",
      "epoch: 1679, loss: -5.782832145690918, rewards: -21.0, count: 1255\n",
      "epoch: 1689, loss: 1.470062017440796, rewards: -20.0, count: 1296\n",
      "epoch: 1699, loss: 3.828789710998535, rewards: -19.0, count: 1608\n",
      "epoch: 1709, loss: 1.3423190116882324, rewards: -20.0, count: 1228\n",
      "epoch: 1719, loss: 2.7451066970825195, rewards: -19.0, count: 1433\n",
      "epoch: 1729, loss: 2.193363904953003, rewards: -19.0, count: 1438\n",
      "epoch: 1739, loss: 2.277857780456543, rewards: -20.0, count: 1289\n",
      "epoch: 1749, loss: -2.1976757049560547, rewards: -21.0, count: 1192\n",
      "epoch: 1759, loss: 2.5937700271606445, rewards: -21.0, count: 1337\n",
      "epoch: 1769, loss: 0.9866752624511719, rewards: -21.0, count: 1167\n",
      "epoch: 1779, loss: 1.2907764911651611, rewards: -20.0, count: 1230\n",
      "epoch: 1789, loss: -1.0249354839324951, rewards: -21.0, count: 1350\n",
      "epoch: 1799, loss: 5.454977512359619, rewards: -21.0, count: 1264\n",
      "epoch: 1809, loss: 1.1890501976013184, rewards: -20.0, count: 1635\n",
      "epoch: 1819, loss: 3.031144618988037, rewards: -20.0, count: 1558\n",
      "epoch: 1829, loss: 0.40421438217163086, rewards: -21.0, count: 1511\n",
      "epoch: 1839, loss: -2.6723899841308594, rewards: -20.0, count: 1222\n",
      "epoch: 1849, loss: -1.3439054489135742, rewards: -21.0, count: 1022\n",
      "epoch: 1859, loss: -1.7173168659210205, rewards: -21.0, count: 1277\n",
      "epoch: 1869, loss: 4.215412616729736, rewards: -20.0, count: 1709\n",
      "epoch: 1879, loss: -0.7836441993713379, rewards: -21.0, count: 1166\n",
      "epoch: 1889, loss: 1.685570240020752, rewards: -21.0, count: 1099\n",
      "epoch: 1899, loss: -0.7853145599365234, rewards: -21.0, count: 1250\n",
      "epoch: 1909, loss: -1.7183024883270264, rewards: -18.0, count: 1497\n",
      "epoch: 1919, loss: 8.834468841552734, rewards: -21.0, count: 1259\n",
      "epoch: 1929, loss: -2.5631024837493896, rewards: -21.0, count: 1613\n",
      "epoch: 1939, loss: 1.1807234287261963, rewards: -19.0, count: 1422\n",
      "epoch: 1949, loss: 1.0329809188842773, rewards: -21.0, count: 1101\n",
      "epoch: 1959, loss: 2.097825765609741, rewards: -20.0, count: 1476\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPong-v4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[133], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m      9\u001b[0m prev_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n\u001b[0;32m---> 11\u001b[0m     diff \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39msample_action(diff)\n\u001b[1;32m     13\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[120], line 21\u001b[0m, in \u001b[0;36mnumpy_to_tensor\u001b[0;34m(numpy_data)\u001b[0m\n\u001b[1;32m     18\u001b[0m tensor_data \u001b[38;5;241m=\u001b[39m numpy_data\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 2. 调整通道顺序 (HWC -> CHW)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m tensor_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 输出形状: (3, 210, 160)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 3. 添加batch维度 (CHW -> BCHW)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m tensor_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(tensor_data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# 输出形状: (1, 3, 210, 160)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:630\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_transpose_dispatcher\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m \n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m'\u001b[39m, axes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v4\")\n",
    "train(agent,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a24b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(self,state):\n",
    "    probs = self.policy_net(state) # 4\n",
    "    if np.random.uniform() < 0.0:\n",
    "        action = np.random.randint(0,2)\n",
    "        return action + 2, torch.log(probs[action]+1e-8)\n",
    "    dist = Categorical(probs)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action.item()+2,log_prob\n",
    "\n",
    "# 替换方法\n",
    "import types\n",
    "agent.sample_action = types.MethodType(sample_action, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ac343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def visualize_agent(env, agent, num_episodes=5):\n",
    "    \"\"\"\n",
    "    渲染显示智能体的行动\n",
    "    \"\"\"\n",
    "    env = gym.make('CliffWalking-v0', render_mode='human')  # 创建可视化环境\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state_tuple = env.reset()\n",
    "        state = state_tuple[0] if isinstance(state_tuple, tuple) else state_tuple\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}\")\n",
    "        \n",
    "        while not done:\n",
    "            env.render()  # 渲染当前状态\n",
    "            \n",
    "            # 将状态转换为one-hot编码\n",
    "            state_onehot = np.zeros(48)\n",
    "            state_onehot[state] = 1\n",
    "            \n",
    "            # 使用训练好的策略选择动作\n",
    "            with torch.no_grad():\n",
    "                if np.random.random() < 0.0:\n",
    "                    action = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    state_tensor = torch.FloatTensor(state_onehot)\n",
    "                    probs = agent.policy_net(state_tensor)\n",
    "                    action = probs.argmax().item()  # 使用最可能的动作\n",
    "            \n",
    "            # 执行动作\n",
    "            step_result = env.step(action)\n",
    "            if len(step_result) == 4:\n",
    "                next_state, reward, done, _ = step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = step_result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # 添加小延迟使动作更容易观察\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(f\"Episode finished after {steps} steps. Total reward: {total_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# 在主程序最后添加：\n",
    "if __name__ == \"__main__\":    \n",
    "    # 训练完成后显示智能体行动\n",
    "    print(\"\\nVisualizing trained agent behavior...\")\n",
    "    env = gym.make('CliffWalking-v0',render_mode='human')\n",
    "    visualize_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1049fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
