{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9d8dda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle  # 用 pickle 替代 cPickle\n",
    "import gym\n",
    "np.bool8 = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "023476ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "581cb90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v4\")\n",
    "observation, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6e40b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(observation):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # 创建一个随机的 (210, 160, 3) 矩阵\n",
    "    image = observation\n",
    "    # image = prepro_1(observation)\n",
    "\n",
    "    # 使用 matplotlib 显示图像\n",
    "    plt.imshow(image)\n",
    "    # plt.axis('off')  # 隐藏坐标轴\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9cd7282d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5dea4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\"将 210x160x3 uint8 帧预处理为 6400 (80x80) 1D float 向量\"\"\"\n",
    "    I = I[35:195]  # 裁剪\n",
    "    I = I[::2, ::2, 0]  # 下采样因子为 2\n",
    "    I[I == 144] = 0  # 删除背景类型 1\n",
    "    I[I == 109] = 0  # 删除背景类型 2\n",
    "    I[I != 0] = 1  # 其他设置为 1\n",
    "    return I.astype(np.float32).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "6c4d3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 6400\n",
    "H = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f85039e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6fbf2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim,200,bias=False)\n",
    "        self.linear2 = nn.Linear(200,output_dim,bias=False)\n",
    "        \n",
    "       # Xavier 初始化 + 偏置归零\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "    def sample_action(self,x):\n",
    "\n",
    "        prob = self.forward(x)\n",
    "#             prob = torch.clamp(prob, min=1e-8, max=1-1e-8)  # 关键修复\n",
    "#             m = Bernoulli(prob)\n",
    "#             action = m.sample().item()  # 1 表示向上，0 表示向下\n",
    "#             return int(action) + 2\n",
    "        action = 0 if prob.item() < np.random.uniform() else 1\n",
    "        return prob, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "240761c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.queue = deque()\n",
    "    \n",
    "    def push(self,x,action,reward,done,y,prob):\n",
    "        self.queue.append((x,action,reward,done,y,prob))\n",
    "    \n",
    "    def clear(self):\n",
    "        self.queue.clear()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.deque)\n",
    "    \n",
    "    def sample(self):\n",
    "        return zip(*self.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "46b18268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim,200)\n",
    "        self.linear2 = nn.Linear(200,output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d6692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "d179875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "44088fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.actor = Actor(D,1)\n",
    "        self.value = Value(D,1) ## 评价当前x\n",
    "        self.actor_optim = AdamW(self.actor.parameters(),lr=1e-3)\n",
    "        self.value_optim = AdamW(self.value.parameters(),lr=1e-3)\n",
    "        self.memory = Memory()\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        \n",
    "        old_xs,old_actions,old_rewards,old_dones,old_ys,old_probs = self.memory.sample()\n",
    "        ### 以上是历史数据，用来训练actor的\n",
    "        \n",
    "        old_xs = torch.tensor(old_xs)           # B,D\n",
    "        old_probs = torch.cat(old_probs,dim=-1)\n",
    "        old_actions = torch.LongTensor(old_actions)\n",
    "        old_ys = torch.tensor(old_ys)           # B,1\n",
    "        \n",
    "        adding = 0\n",
    "        returns = []\n",
    "        for reward,done in reversed(list(zip(old_rewards,old_dones))):\n",
    "            if done:\n",
    "                adding = 0\n",
    "            adding = adding * 0.99 + reward\n",
    "            returns.insert(0,adding)\n",
    "        \n",
    "        returns = torch.tensor(returns) # B,1?\n",
    "        returns -= returns.mean()\n",
    "        returns /= (returns.std()+1e-5)\n",
    "        \n",
    "  \n",
    "        probs = self.actor(old_xs) \n",
    "        log_probs = (1-old_ys)*torch.log(1-probs) + old_ys * torch.log(probs)\n",
    "        actor_loss = (returns * log_probs).mean()\n",
    "        print('loss',actor_loss,(returns * log_probs).sum(),len(returns))\n",
    "        \n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "#         for _ in range(4):\n",
    "#             values = self.value(old_xs) ### B,2\n",
    "# #             vs = values.gather(dim=-1,index=old_actions.unsqueeze(dim=-1)) # B,1\n",
    "#             ### 根据以前的动作来拟合评价标准\n",
    "#             advantages = (returns - values).detach()\n",
    "            \n",
    "#             probs = self.actor(old_xs) ### \n",
    "#             ### 根据新的action选择\n",
    "#             log_probs = (1-old_ys)*torch.log(1-probs) + old_ys * torch.log(probs) # B,1\n",
    "#             old_logprobs = (1-old_ys)*torch.log(1-old_probs) + old_ys * torch.log(old_probs)\n",
    "            \n",
    "#             ratio = torch.exp(log_probs - old_logprobs)\n",
    "#             clamp = torch.clamp(ratio,0.8,1.2)\n",
    "#             actor_loss = -torch.min(advantages * clamp, advantages * ratio).mean()\n",
    "#             value_loss = (returns - values).pow(2).mean()\n",
    "        \n",
    "            \n",
    "#             self.actor_optim.zero_grad()\n",
    "#             actor_loss.backward()\n",
    "#             self.actor_optim.step()\n",
    "            \n",
    "#             self.value_optim.zero_grad()\n",
    "#             value_loss.backward()\n",
    "#             self.value_optim.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63effb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5b818012",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e84d5e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.0004, grad_fn=<MeanBackward0>) tensor(893.1169, grad_fn=<SumBackward0>) 1471\n",
      "total -20.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(246.0352, grad_fn=<SumBackward0>) 1218\n",
      "total -20.0\n",
      "loss tensor(0.0005, grad_fn=<MeanBackward0>) tensor(810.5383, grad_fn=<SumBackward0>) 1250\n",
      "total -21.0\n",
      "loss tensor(-0.0002, grad_fn=<MeanBackward0>) tensor(-186.8900, grad_fn=<SumBackward0>) 1075\n",
      "total -21.0\n",
      "loss tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(-818.5081, grad_fn=<SumBackward0>) 1186\n",
      "total -21.0\n",
      "loss tensor(-0.0005, grad_fn=<MeanBackward0>) tensor(-812.3563, grad_fn=<SumBackward0>) 1257\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-430.6109, grad_fn=<SumBackward0>) 1251\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(343.3074, grad_fn=<SumBackward0>) 1261\n",
      "total -21.0\n",
      "loss tensor(-0.0032, grad_fn=<MeanBackward0>) tensor(-3940.3386, grad_fn=<SumBackward0>) 1105\n",
      "total -21.0\n",
      "loss tensor(-0.0017, grad_fn=<MeanBackward0>) tensor(-1748.4429, grad_fn=<SumBackward0>) 1003\n",
      "total -21.0\n",
      "loss tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(-2306.0195, grad_fn=<SumBackward0>) 1101\n",
      "total -21.0\n",
      "loss tensor(-0.0012, grad_fn=<MeanBackward0>) tensor(-1230.9709, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0026, grad_fn=<MeanBackward0>) tensor(2652.5325, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(0.0034, grad_fn=<MeanBackward0>) tensor(3631.2046, grad_fn=<SumBackward0>) 1033\n",
      "total -21.0\n",
      "loss tensor(0.0025, grad_fn=<MeanBackward0>) tensor(3067.4111, grad_fn=<SumBackward0>) 1110\n",
      "total -21.0\n",
      "loss tensor(-0.0049, grad_fn=<MeanBackward0>) tensor(-8240.8428, grad_fn=<SumBackward0>) 1302\n",
      "total -20.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-3150.8772, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(-0.0033, grad_fn=<MeanBackward0>) tensor(-3504.3513, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(-0.0099, grad_fn=<MeanBackward0>) tensor(-13343.2275, grad_fn=<SumBackward0>) 1162\n",
      "total -21.0\n",
      "loss tensor(-0.0047, grad_fn=<MeanBackward0>) tensor(-4857.4409, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0043, grad_fn=<MeanBackward0>) tensor(4317.6987, grad_fn=<SumBackward0>) 1004\n",
      "total -21.0\n",
      "loss tensor(-0.0070, grad_fn=<MeanBackward0>) tensor(-7253.6108, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0016, grad_fn=<MeanBackward0>) tensor(-1665.2462, grad_fn=<SumBackward0>) 1008\n",
      "total -21.0\n",
      "loss tensor(-0.0068, grad_fn=<MeanBackward0>) tensor(-6965.2339, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(0.0093, grad_fn=<MeanBackward0>) tensor(10070.1631, grad_fn=<SumBackward0>) 1039\n",
      "total -21.0\n",
      "loss tensor(-0.0076, grad_fn=<MeanBackward0>) tensor(-7727.5425, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(-0.0026, grad_fn=<MeanBackward0>) tensor(-2642.7639, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(-0.0070, grad_fn=<MeanBackward0>) tensor(-9637.8730, grad_fn=<SumBackward0>) 1175\n",
      "total -21.0\n",
      "loss tensor(-0.0070, grad_fn=<MeanBackward0>) tensor(-7301.9272, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0071, grad_fn=<MeanBackward0>) tensor(7287.7095, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(-0.0057, grad_fn=<MeanBackward0>) tensor(-5976.9722, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(0.0025, grad_fn=<MeanBackward0>) tensor(2584.5984, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0123, grad_fn=<MeanBackward0>) tensor(12758.8320, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0126, grad_fn=<MeanBackward0>) tensor(-12849.3818, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(0.0036, grad_fn=<MeanBackward0>) tensor(3639.0312, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(0.0010, grad_fn=<MeanBackward0>) tensor(1198.5735, grad_fn=<SumBackward0>) 1098\n",
      "total -21.0\n",
      "loss tensor(0.0122, grad_fn=<MeanBackward0>) tensor(12680.1963, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(0.0029, grad_fn=<MeanBackward0>) tensor(2968.9900, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0006, grad_fn=<MeanBackward0>) tensor(641.5248, grad_fn=<SumBackward0>) 1002\n",
      "total -21.0\n",
      "loss tensor(0.0138, grad_fn=<MeanBackward0>) tensor(14171.5166, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(0.0023, grad_fn=<MeanBackward0>) tensor(2422.7356, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(-0.0172, grad_fn=<MeanBackward0>) tensor(-17748.5762, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0103, grad_fn=<MeanBackward0>) tensor(10834.8809, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0026, grad_fn=<MeanBackward0>) tensor(2589.9922, grad_fn=<SumBackward0>) 1006\n",
      "total -21.0\n",
      "loss tensor(0.0106, grad_fn=<MeanBackward0>) tensor(11465.4258, grad_fn=<SumBackward0>) 1039\n",
      "total -21.0\n",
      "loss tensor(-0.0065, grad_fn=<MeanBackward0>) tensor(-6762.8159, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0033, grad_fn=<MeanBackward0>) tensor(-3314.0088, grad_fn=<SumBackward0>) 1008\n",
      "total -21.0\n",
      "loss tensor(-0.0042, grad_fn=<MeanBackward0>) tensor(-4300.5322, grad_fn=<SumBackward0>) 1011\n",
      "total -21.0\n",
      "loss tensor(-0.0054, grad_fn=<MeanBackward0>) tensor(-5568.6587, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(-0.0094, grad_fn=<MeanBackward0>) tensor(-9724.6211, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1087.4102, grad_fn=<SumBackward0>) 1004\n",
      "total -21.0\n",
      "loss tensor(-0.0001, grad_fn=<MeanBackward0>) tensor(-104.6954, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(0.0150, grad_fn=<MeanBackward0>) tensor(15464.4160, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(0.0011, grad_fn=<MeanBackward0>) tensor(1122.0881, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0016, grad_fn=<MeanBackward0>) tensor(1701.0746, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0101, grad_fn=<MeanBackward0>) tensor(10415.0469, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0088, grad_fn=<MeanBackward0>) tensor(9087.4844, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0035, grad_fn=<MeanBackward0>) tensor(-3567.0444, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0047, grad_fn=<MeanBackward0>) tensor(4811.7256, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(0.0058, grad_fn=<MeanBackward0>) tensor(6016.8105, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0058, grad_fn=<MeanBackward0>) tensor(6026.0117, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(0.0075, grad_fn=<MeanBackward0>) tensor(7646.0786, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(-0.0106, grad_fn=<MeanBackward0>) tensor(-15236.8457, grad_fn=<SumBackward0>) 1197\n",
      "total -21.0\n",
      "loss tensor(0.0153, grad_fn=<MeanBackward0>) tensor(16064.9004, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0053, grad_fn=<MeanBackward0>) tensor(5590.5962, grad_fn=<SumBackward0>) 1030\n",
      "total -21.0\n",
      "loss tensor(0.0021, grad_fn=<MeanBackward0>) tensor(2179.3013, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-323.6810, grad_fn=<SumBackward0>) 1032\n",
      "total -21.0\n",
      "loss tensor(0.0031, grad_fn=<MeanBackward0>) tensor(4496.7876, grad_fn=<SumBackward0>) 1208\n",
      "total -20.0\n",
      "loss tensor(-0.0080, grad_fn=<MeanBackward0>) tensor(-9740.1162, grad_fn=<SumBackward0>) 1104\n",
      "total -21.0\n",
      "loss tensor(-0.0036, grad_fn=<MeanBackward0>) tensor(-6907.8926, grad_fn=<SumBackward0>) 1386\n",
      "total -20.0\n",
      "loss tensor(0.0036, grad_fn=<MeanBackward0>) tensor(3736.4028, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0014, grad_fn=<MeanBackward0>) tensor(-1510.8879, grad_fn=<SumBackward0>) 1035\n",
      "total -21.0\n",
      "loss tensor(-0.0045, grad_fn=<MeanBackward0>) tensor(-5616.2026, grad_fn=<SumBackward0>) 1119\n",
      "total -21.0\n",
      "loss tensor(-0.0029, grad_fn=<MeanBackward0>) tensor(-3585.1992, grad_fn=<SumBackward0>) 1114\n",
      "total -21.0\n",
      "loss tensor(0.0015, grad_fn=<MeanBackward0>) tensor(1522.4358, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(-0.0060, grad_fn=<MeanBackward0>) tensor(-6180.9941, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(0.0030, grad_fn=<MeanBackward0>) tensor(3188.2451, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0018, grad_fn=<MeanBackward0>) tensor(1834.5957, grad_fn=<SumBackward0>) 1011\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(232.6278, grad_fn=<SumBackward0>) 1029\n",
      "total -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(-0.0051, grad_fn=<MeanBackward0>) tensor(-5200.9946, grad_fn=<SumBackward0>) 1005\n",
      "total -21.0\n",
      "loss tensor(0.0030, grad_fn=<MeanBackward0>) tensor(3658.4241, grad_fn=<SumBackward0>) 1102\n",
      "total -21.0\n",
      "loss tensor(-0.0020, grad_fn=<MeanBackward0>) tensor(-2063.6299, grad_fn=<SumBackward0>) 1005\n",
      "total -21.0\n",
      "loss tensor(0.0007, grad_fn=<MeanBackward0>) tensor(687.0309, grad_fn=<SumBackward0>) 1008\n",
      "total -21.0\n",
      "loss tensor(0.0073, grad_fn=<MeanBackward0>) tensor(7540.6499, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(0.0064, grad_fn=<MeanBackward0>) tensor(6747.4512, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(0.0046, grad_fn=<MeanBackward0>) tensor(4951.8921, grad_fn=<SumBackward0>) 1036\n",
      "total -21.0\n",
      "loss tensor(-0.0065, grad_fn=<MeanBackward0>) tensor(-6832.0693, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0033, grad_fn=<MeanBackward0>) tensor(-3224.8845, grad_fn=<SumBackward0>) 994\n",
      "total -21.0\n",
      "loss tensor(0.0051, grad_fn=<MeanBackward0>) tensor(5206.3345, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(-0.0025, grad_fn=<MeanBackward0>) tensor(-2579.5596, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(0.0042, grad_fn=<MeanBackward0>) tensor(4946.7446, grad_fn=<SumBackward0>) 1089\n",
      "total -21.0\n",
      "loss tensor(0.0001, grad_fn=<MeanBackward0>) tensor(154.8014, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(0.0021, grad_fn=<MeanBackward0>) tensor(2488.0012, grad_fn=<SumBackward0>) 1101\n",
      "total -21.0\n",
      "loss tensor(0.0003, grad_fn=<MeanBackward0>) tensor(345.4761, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0001, grad_fn=<MeanBackward0>) tensor(137.6991, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0026, grad_fn=<MeanBackward0>) tensor(2649.3608, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0035, grad_fn=<MeanBackward0>) tensor(3666.0527, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0043, grad_fn=<MeanBackward0>) tensor(-4375.0947, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-390.9935, grad_fn=<SumBackward0>) 1107\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(1461.6904, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0040, grad_fn=<MeanBackward0>) tensor(4062.9995, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(-0.0015, grad_fn=<MeanBackward0>) tensor(-1522.4404, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1984.4778, grad_fn=<SumBackward0>) 1371\n",
      "total -20.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1513.9987, grad_fn=<SumBackward0>) 1173\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-1614.5424, grad_fn=<SumBackward0>) 1096\n",
      "total -21.0\n",
      "loss tensor(0.0007, grad_fn=<MeanBackward0>) tensor(709.2941, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(0.0012, grad_fn=<MeanBackward0>) tensor(1160.7264, grad_fn=<SumBackward0>) 997\n",
      "total -21.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1568.3705, grad_fn=<SumBackward0>) 1218\n",
      "total -20.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-5191.6323, grad_fn=<SumBackward0>) 1307\n",
      "total -20.0\n",
      "loss tensor(0.0007, grad_fn=<MeanBackward0>) tensor(1554.8789, grad_fn=<SumBackward0>) 1511\n",
      "total -19.0\n",
      "loss tensor(0.0006, grad_fn=<MeanBackward0>) tensor(768.0541, grad_fn=<SumBackward0>) 1179\n",
      "total -21.0\n",
      "loss tensor(0.0028, grad_fn=<MeanBackward0>) tensor(5551.3135, grad_fn=<SumBackward0>) 1401\n",
      "total -20.0\n",
      "loss tensor(0.0012, grad_fn=<MeanBackward0>) tensor(1946.4968, grad_fn=<SumBackward0>) 1261\n",
      "total -21.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1089.2942, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-372.7408, grad_fn=<SumBackward0>) 1101\n",
      "total -21.0\n",
      "loss tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(-1055.4144, grad_fn=<SumBackward0>) 1302\n",
      "total -20.0\n",
      "loss tensor(0.0006, grad_fn=<MeanBackward0>) tensor(656.2654, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(-0.0012, grad_fn=<MeanBackward0>) tensor(-1240.5804, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(7.4116e-05, grad_fn=<MeanBackward0>) tensor(103.1992, grad_fn=<SumBackward0>) 1180\n",
      "total -21.0\n",
      "loss tensor(0.0004, grad_fn=<MeanBackward0>) tensor(813.8376, grad_fn=<SumBackward0>) 1392\n",
      "total -20.0\n",
      "loss tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(-670.5167, grad_fn=<SumBackward0>) 1091\n",
      "total -21.0\n",
      "loss tensor(0.0005, grad_fn=<MeanBackward0>) tensor(529.0305, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(-3703.0144, grad_fn=<SumBackward0>) 1385\n",
      "total -20.0\n",
      "loss tensor(0.0004, grad_fn=<MeanBackward0>) tensor(446.4852, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(0.0007, grad_fn=<MeanBackward0>) tensor(704.0748, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(-0.0034, grad_fn=<MeanBackward0>) tensor(-5388.4795, grad_fn=<SumBackward0>) 1260\n",
      "total -21.0\n",
      "loss tensor(0.0025, grad_fn=<MeanBackward0>) tensor(5243.9365, grad_fn=<SumBackward0>) 1462\n",
      "total -20.0\n",
      "loss tensor(0.0023, grad_fn=<MeanBackward0>) tensor(2851.4326, grad_fn=<SumBackward0>) 1105\n",
      "total -21.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-4705.6821, grad_fn=<SumBackward0>) 1247\n",
      "total -21.0\n",
      "loss tensor(-0.0012, grad_fn=<MeanBackward0>) tensor(-1653.1908, grad_fn=<SumBackward0>) 1177\n",
      "total -21.0\n",
      "loss tensor(-0.0026, grad_fn=<MeanBackward0>) tensor(-3117.8286, grad_fn=<SumBackward0>) 1096\n",
      "total -21.0\n",
      "loss tensor(-0.0002, grad_fn=<MeanBackward0>) tensor(-609.0558, grad_fn=<SumBackward0>) 1658\n",
      "total -20.0\n",
      "loss tensor(-0.0012, grad_fn=<MeanBackward0>) tensor(-1221.6099, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-3964.9775, grad_fn=<SumBackward0>) 1344\n",
      "total -21.0\n",
      "loss tensor(0.0077, grad_fn=<MeanBackward0>) tensor(7821.4268, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(-0.0031, grad_fn=<MeanBackward0>) tensor(-4228.7061, grad_fn=<SumBackward0>) 1169\n",
      "total -21.0\n",
      "loss tensor(0.0007, grad_fn=<MeanBackward0>) tensor(879.5360, grad_fn=<SumBackward0>) 1090\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-1341.2113, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(0.0005, grad_fn=<MeanBackward0>) tensor(542.0536, grad_fn=<SumBackward0>) 1039\n",
      "total -21.0\n",
      "loss tensor(0.0030, grad_fn=<MeanBackward0>) tensor(3086.9646, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1191.4796, grad_fn=<SumBackward0>) 1031\n",
      "total -21.0\n",
      "loss tensor(0.0028, grad_fn=<MeanBackward0>) tensor(3361.9829, grad_fn=<SumBackward0>) 1088\n",
      "total -21.0\n",
      "loss tensor(0.0027, grad_fn=<MeanBackward0>) tensor(2799.5813, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-1771.8209, grad_fn=<SumBackward0>) 1175\n",
      "total -21.0\n",
      "loss tensor(-0.0015, grad_fn=<MeanBackward0>) tensor(-1861.1707, grad_fn=<SumBackward0>) 1105\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(1480.3093, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-2247.8025, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-726.2296, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0022, grad_fn=<MeanBackward0>) tensor(3615.3674, grad_fn=<SumBackward0>) 1268\n",
      "total -21.0\n",
      "loss tensor(0.0011, grad_fn=<MeanBackward0>) tensor(1162.7887, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-854.6935, grad_fn=<SumBackward0>) 1098\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(2272.5627, grad_fn=<SumBackward0>) 1262\n",
      "total -21.0\n",
      "loss tensor(0.0026, grad_fn=<MeanBackward0>) tensor(2813.8879, grad_fn=<SumBackward0>) 1036\n",
      "total -21.0\n",
      "loss tensor(-0.0008, grad_fn=<MeanBackward0>) tensor(-1007.5698, grad_fn=<SumBackward0>) 1097\n",
      "total -21.0\n",
      "loss tensor(-0.0017, grad_fn=<MeanBackward0>) tensor(-1782.3380, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(0.0022, grad_fn=<MeanBackward0>) tensor(2341.2905, grad_fn=<SumBackward0>) 1033\n",
      "total -21.0\n",
      "loss tensor(-0.0012, grad_fn=<MeanBackward0>) tensor(-1398.3722, grad_fn=<SumBackward0>) 1088\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(227.4215, grad_fn=<SumBackward0>) 1005\n",
      "total -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.0015, grad_fn=<MeanBackward0>) tensor(2846.3237, grad_fn=<SumBackward0>) 1395\n",
      "total -20.0\n",
      "loss tensor(-0.0058, grad_fn=<MeanBackward0>) tensor(-9946.4238, grad_fn=<SumBackward0>) 1313\n",
      "total -20.0\n",
      "loss tensor(0.0018, grad_fn=<MeanBackward0>) tensor(3536.9934, grad_fn=<SumBackward0>) 1388\n",
      "total -20.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-846.9476, grad_fn=<SumBackward0>) 1106\n",
      "total -21.0\n",
      "loss tensor(-0.0018, grad_fn=<MeanBackward0>) tensor(-2211.0715, grad_fn=<SumBackward0>) 1096\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-3520.9465, grad_fn=<SumBackward0>) 1258\n",
      "total -21.0\n",
      "loss tensor(-0.0009, grad_fn=<MeanBackward0>) tensor(-1104.9182, grad_fn=<SumBackward0>) 1099\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(176.9655, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(319.1348, grad_fn=<SumBackward0>) 1256\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-1347.6134, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(-0.0016, grad_fn=<MeanBackward0>) tensor(-1671.5254, grad_fn=<SumBackward0>) 1011\n",
      "total -21.0\n",
      "loss tensor(-0.0023, grad_fn=<MeanBackward0>) tensor(-2762.7078, grad_fn=<SumBackward0>) 1107\n",
      "total -21.0\n",
      "loss tensor(-0.0059, grad_fn=<MeanBackward0>) tensor(-6855.5610, grad_fn=<SumBackward0>) 1081\n",
      "total -21.0\n",
      "loss tensor(-0.0015, grad_fn=<MeanBackward0>) tensor(-1544.4335, grad_fn=<SumBackward0>) 1031\n",
      "total -21.0\n",
      "loss tensor(-0.0017, grad_fn=<MeanBackward0>) tensor(-1786.8315, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0053, grad_fn=<MeanBackward0>) tensor(5575.5454, grad_fn=<SumBackward0>) 1029\n",
      "total -21.0\n",
      "loss tensor(0.0020, grad_fn=<MeanBackward0>) tensor(2030.2104, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(0.0058, grad_fn=<MeanBackward0>) tensor(6036.4688, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0023, grad_fn=<MeanBackward0>) tensor(-2746.1257, grad_fn=<SumBackward0>) 1101\n",
      "total -21.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-3146.3247, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0065, grad_fn=<MeanBackward0>) tensor(6768.8413, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(-474.6151, grad_fn=<SumBackward0>) 1034\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-2194.8110, grad_fn=<SumBackward0>) 1278\n",
      "total -21.0\n",
      "loss tensor(0.0021, grad_fn=<MeanBackward0>) tensor(2301.4136, grad_fn=<SumBackward0>) 1038\n",
      "total -21.0\n",
      "loss tensor(-0.0070, grad_fn=<MeanBackward0>) tensor(-6973.9810, grad_fn=<SumBackward0>) 1001\n",
      "total -21.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-3037.8533, grad_fn=<SumBackward0>) 999\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-2298.1558, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1159.2195, grad_fn=<SumBackward0>) 1032\n",
      "total -21.0\n",
      "loss tensor(-0.0052, grad_fn=<MeanBackward0>) tensor(-5574.7568, grad_fn=<SumBackward0>) 1034\n",
      "total -21.0\n",
      "loss tensor(0.0040, grad_fn=<MeanBackward0>) tensor(4304.6895, grad_fn=<SumBackward0>) 1031\n",
      "total -21.0\n",
      "loss tensor(0.0008, grad_fn=<MeanBackward0>) tensor(811.2473, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0053, grad_fn=<MeanBackward0>) tensor(-5558.6055, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0061, grad_fn=<MeanBackward0>) tensor(6337.6045, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-365.2592, grad_fn=<SumBackward0>) 1036\n",
      "total -21.0\n",
      "loss tensor(-0.0028, grad_fn=<MeanBackward0>) tensor(-2864.0115, grad_fn=<SumBackward0>) 1003\n",
      "total -21.0\n",
      "loss tensor(0.0010, grad_fn=<MeanBackward0>) tensor(1090.0992, grad_fn=<SumBackward0>) 1032\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-262.9737, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(-0.0014, grad_fn=<MeanBackward0>) tensor(-1413.2600, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0077, grad_fn=<MeanBackward0>) tensor(-8112.2363, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(-0.0014, grad_fn=<MeanBackward0>) tensor(-1412.8846, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(0.0044, grad_fn=<MeanBackward0>) tensor(4662.3638, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0027, grad_fn=<MeanBackward0>) tensor(-2652.9529, grad_fn=<SumBackward0>) 998\n",
      "total -21.0\n",
      "loss tensor(0.0116, grad_fn=<MeanBackward0>) tensor(12150.8535, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-360.2987, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0087, grad_fn=<MeanBackward0>) tensor(9092.9717, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(161.2439, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-6.3046e-05, grad_fn=<MeanBackward0>) tensor(-63.5509, grad_fn=<SumBackward0>) 1004\n",
      "total -21.0\n",
      "loss tensor(0.0065, grad_fn=<MeanBackward0>) tensor(6581.8491, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(0.0118, grad_fn=<MeanBackward0>) tensor(12590.6367, grad_fn=<SumBackward0>) 1033\n",
      "total -21.0\n",
      "loss tensor(0.0017, grad_fn=<MeanBackward0>) tensor(1759.1005, grad_fn=<SumBackward0>) 1011\n",
      "total -21.0\n",
      "loss tensor(0.0033, grad_fn=<MeanBackward0>) tensor(3339.9731, grad_fn=<SumBackward0>) 1006\n",
      "total -21.0\n",
      "loss tensor(-0.0050, grad_fn=<MeanBackward0>) tensor(-5993.2349, grad_fn=<SumBackward0>) 1096\n",
      "total -21.0\n",
      "loss tensor(0.0019, grad_fn=<MeanBackward0>) tensor(1985.9048, grad_fn=<SumBackward0>) 1030\n",
      "total -21.0\n",
      "loss tensor(0.0008, grad_fn=<MeanBackward0>) tensor(823.7164, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0040, grad_fn=<MeanBackward0>) tensor(-4229.9067, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0075, grad_fn=<MeanBackward0>) tensor(7721.5791, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(0.0015, grad_fn=<MeanBackward0>) tensor(1481.7429, grad_fn=<SumBackward0>) 1003\n",
      "total -21.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-764.0146, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0048, grad_fn=<MeanBackward0>) tensor(-4843.1777, grad_fn=<SumBackward0>) 1006\n",
      "total -21.0\n",
      "loss tensor(-0.0026, grad_fn=<MeanBackward0>) tensor(-3189.6897, grad_fn=<SumBackward0>) 1100\n",
      "total -21.0\n",
      "loss tensor(0.0022, grad_fn=<MeanBackward0>) tensor(2247.7681, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(-0.0044, grad_fn=<MeanBackward0>) tensor(-4500.9512, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0029, grad_fn=<MeanBackward0>) tensor(-2972.1792, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0083, grad_fn=<MeanBackward0>) tensor(-9889.8877, grad_fn=<SumBackward0>) 1093\n",
      "total -21.0\n",
      "loss tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(-2018.3512, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(-0.0016, grad_fn=<MeanBackward0>) tensor(-1677.5258, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(0.0035, grad_fn=<MeanBackward0>) tensor(3596.9080, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(9.4895e-05, grad_fn=<MeanBackward0>) tensor(100.4783, grad_fn=<SumBackward0>) 1029\n",
      "total -21.0\n",
      "loss tensor(0.0007, grad_fn=<MeanBackward0>) tensor(759.9696, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(1511.1748, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(-0.0041, grad_fn=<MeanBackward0>) tensor(-4191.7178, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0047, grad_fn=<MeanBackward0>) tensor(4857.4585, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(-0.0020, grad_fn=<MeanBackward0>) tensor(-2024.3391, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(0.0025, grad_fn=<MeanBackward0>) tensor(2605.1809, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(0.0034, grad_fn=<MeanBackward0>) tensor(3527.5303, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(0.0055, grad_fn=<MeanBackward0>) tensor(5587.8501, grad_fn=<SumBackward0>) 1008\n",
      "total -21.0\n",
      "loss tensor(-0.0051, grad_fn=<MeanBackward0>) tensor(-5223.6855, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(0.0012, grad_fn=<MeanBackward0>) tensor(1275.2271, grad_fn=<SumBackward0>) 1030\n",
      "total -21.0\n",
      "loss tensor(-0.0047, grad_fn=<MeanBackward0>) tensor(-5115.2158, grad_fn=<SumBackward0>) 1042\n",
      "total -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(-0.0044, grad_fn=<MeanBackward0>) tensor(-5054.8579, grad_fn=<SumBackward0>) 1077\n",
      "total -21.0\n",
      "loss tensor(0.0049, grad_fn=<MeanBackward0>) tensor(5087.5200, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0021, grad_fn=<MeanBackward0>) tensor(-2244.4966, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(0.0044, grad_fn=<MeanBackward0>) tensor(4543.6680, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0054, grad_fn=<MeanBackward0>) tensor(-5655.7876, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(-1961.2551, grad_fn=<SumBackward0>) 1029\n",
      "total -21.0\n",
      "loss tensor(0.0030, grad_fn=<MeanBackward0>) tensor(3042.0649, grad_fn=<SumBackward0>) 1004\n",
      "total -21.0\n",
      "loss tensor(0.0097, grad_fn=<MeanBackward0>) tensor(9865.9609, grad_fn=<SumBackward0>) 1006\n",
      "total -21.0\n",
      "loss tensor(-0.0068, grad_fn=<MeanBackward0>) tensor(-7103.9976, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0017, grad_fn=<MeanBackward0>) tensor(-1764.8926, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0049, grad_fn=<MeanBackward0>) tensor(5212.2148, grad_fn=<SumBackward0>) 1032\n",
      "total -21.0\n",
      "loss tensor(-0.0023, grad_fn=<MeanBackward0>) tensor(-2382.8569, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(-0.0082, grad_fn=<MeanBackward0>) tensor(-8447.6035, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(8.0967e-05, grad_fn=<MeanBackward0>) tensor(82.2674, grad_fn=<SumBackward0>) 1008\n",
      "total -21.0\n",
      "loss tensor(0.0028, grad_fn=<MeanBackward0>) tensor(2879.9656, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-2267.7930, grad_fn=<SumBackward0>) 1008\n",
      "total -21.0\n",
      "loss tensor(-0.0039, grad_fn=<MeanBackward0>) tensor(-4124.1704, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(0.0079, grad_fn=<MeanBackward0>) tensor(8225.0098, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0016, grad_fn=<MeanBackward0>) tensor(-1672.6310, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(-0.0005, grad_fn=<MeanBackward0>) tensor(-562.6048, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(0.0065, grad_fn=<MeanBackward0>) tensor(6769.6938, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0037, grad_fn=<MeanBackward0>) tensor(-3840.7849, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(1490.9111, grad_fn=<SumBackward0>) 1031\n",
      "total -21.0\n",
      "loss tensor(-0.0014, grad_fn=<MeanBackward0>) tensor(-1443.6589, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(1455.4817, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(0.0021, grad_fn=<MeanBackward0>) tensor(2209.3926, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0010, grad_fn=<MeanBackward0>) tensor(1065.2960, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(224.3035, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(-0.0024, grad_fn=<MeanBackward0>) tensor(-2552.7739, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(0.0018, grad_fn=<MeanBackward0>) tensor(1954.3738, grad_fn=<SumBackward0>) 1033\n",
      "total -21.0\n",
      "loss tensor(0.0076, grad_fn=<MeanBackward0>) tensor(8017.4365, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(-0.0029, grad_fn=<MeanBackward0>) tensor(-3043.9792, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(0.0044, grad_fn=<MeanBackward0>) tensor(4671.2412, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(0.0078, grad_fn=<MeanBackward0>) tensor(8007.8799, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(-0.0139, grad_fn=<MeanBackward0>) tensor(-27903.5742, grad_fn=<SumBackward0>) 1419\n",
      "total -21.0\n",
      "loss tensor(-0.0020, grad_fn=<MeanBackward0>) tensor(-2082.2529, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0031, grad_fn=<MeanBackward0>) tensor(-3195.5464, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0010, grad_fn=<MeanBackward0>) tensor(1042.8298, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0027, grad_fn=<MeanBackward0>) tensor(2808.8428, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(-0.0033, grad_fn=<MeanBackward0>) tensor(-3370.7275, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(0.0040, grad_fn=<MeanBackward0>) tensor(4207.9580, grad_fn=<SumBackward0>) 1032\n",
      "total -21.0\n",
      "loss tensor(0.0020, grad_fn=<MeanBackward0>) tensor(2060.9419, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-330.8960, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0008, grad_fn=<MeanBackward0>) tensor(-826.5703, grad_fn=<SumBackward0>) 1003\n",
      "total -21.0\n",
      "loss tensor(-0.0032, grad_fn=<MeanBackward0>) tensor(-3251.9968, grad_fn=<SumBackward0>) 1001\n",
      "total -21.0\n",
      "loss tensor(0.0019, grad_fn=<MeanBackward0>) tensor(2000.6812, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0029, grad_fn=<MeanBackward0>) tensor(3023.4358, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(0.0027, grad_fn=<MeanBackward0>) tensor(2808.4639, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(0.0017, grad_fn=<MeanBackward0>) tensor(1675.7642, grad_fn=<SumBackward0>) 1005\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-1357.3630, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(0.0021, grad_fn=<MeanBackward0>) tensor(2204.3743, grad_fn=<SumBackward0>) 1031\n",
      "total -21.0\n",
      "loss tensor(-0.0040, grad_fn=<MeanBackward0>) tensor(-4155.2832, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(0.0004, grad_fn=<MeanBackward0>) tensor(452.7934, grad_fn=<SumBackward0>) 1089\n",
      "total -21.0\n",
      "loss tensor(0.0012, grad_fn=<MeanBackward0>) tensor(1229.9603, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0036, grad_fn=<MeanBackward0>) tensor(3772.5784, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(2200.1689, grad_fn=<SumBackward0>) 1232\n",
      "total -20.0\n",
      "loss tensor(0.0042, grad_fn=<MeanBackward0>) tensor(4268.7905, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(-2270.7014, grad_fn=<SumBackward0>) 1094\n",
      "total -21.0\n",
      "loss tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(-1135.0507, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0004, grad_fn=<MeanBackward0>) tensor(376.7593, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(0.0031, grad_fn=<MeanBackward0>) tensor(3279.8494, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(0.0010, grad_fn=<MeanBackward0>) tensor(1192.3347, grad_fn=<SumBackward0>) 1085\n",
      "total -21.0\n",
      "loss tensor(-0.0002, grad_fn=<MeanBackward0>) tensor(-181.9738, grad_fn=<SumBackward0>) 1088\n",
      "total -21.0\n",
      "loss tensor(0.0008, grad_fn=<MeanBackward0>) tensor(1147.6453, grad_fn=<SumBackward0>) 1169\n",
      "total -21.0\n",
      "loss tensor(0.0004, grad_fn=<MeanBackward0>) tensor(429.8916, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-1563.2911, grad_fn=<SumBackward0>) 1106\n",
      "total -21.0\n",
      "loss tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(-520.2655, grad_fn=<SumBackward0>) 1100\n",
      "total -21.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-1203.4609, grad_fn=<SumBackward0>) 1336\n",
      "total -21.0\n",
      "loss tensor(-0.0020, grad_fn=<MeanBackward0>) tensor(-2375.2588, grad_fn=<SumBackward0>) 1094\n",
      "total -21.0\n",
      "loss tensor(-0.0027, grad_fn=<MeanBackward0>) tensor(-4336.4512, grad_fn=<SumBackward0>) 1261\n",
      "total -21.0\n",
      "loss tensor(-0.0083, grad_fn=<MeanBackward0>) tensor(-17316.8281, grad_fn=<SumBackward0>) 1445\n",
      "total -20.0\n",
      "loss tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(-608.0773, grad_fn=<SumBackward0>) 1032\n",
      "total -21.0\n",
      "loss tensor(0.0005, grad_fn=<MeanBackward0>) tensor(559.0421, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-3105.9927, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0018, grad_fn=<MeanBackward0>) tensor(1887.2052, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(-0.0034, grad_fn=<MeanBackward0>) tensor(-5397.4702, grad_fn=<SumBackward0>) 1258\n",
      "total -21.0\n",
      "loss tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(-371.4530, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(6.5534e-05, grad_fn=<MeanBackward0>) tensor(68.9865, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(-0.0020, grad_fn=<MeanBackward0>) tensor(-2049.1143, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(-0.0014, grad_fn=<MeanBackward0>) tensor(-1471.2625, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0010, grad_fn=<MeanBackward0>) tensor(-1068.0184, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0010, grad_fn=<MeanBackward0>) tensor(-997.7277, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0047, grad_fn=<MeanBackward0>) tensor(4773.5464, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(0.0050, grad_fn=<MeanBackward0>) tensor(5198.6304, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0003, grad_fn=<MeanBackward0>) tensor(333.6965, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0009, grad_fn=<MeanBackward0>) tensor(-931.3029, grad_fn=<SumBackward0>) 1003\n",
      "total -21.0\n",
      "loss tensor(0.0034, grad_fn=<MeanBackward0>) tensor(3523.6863, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(0.0008, grad_fn=<MeanBackward0>) tensor(826.4037, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0009, grad_fn=<MeanBackward0>) tensor(-920.6905, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0032, grad_fn=<MeanBackward0>) tensor(-3445.0901, grad_fn=<SumBackward0>) 1036\n",
      "total -21.0\n",
      "loss tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(-2002.3263, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(-0.0029, grad_fn=<MeanBackward0>) tensor(-3035.3657, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(0.0006, grad_fn=<MeanBackward0>) tensor(621.2363, grad_fn=<SumBackward0>) 1016\n",
      "total -21.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-3114.7087, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-3253.1562, grad_fn=<SumBackward0>) 1035\n",
      "total -21.0\n",
      "loss tensor(0.0013, grad_fn=<MeanBackward0>) tensor(1304.6881, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(-0.0044, grad_fn=<MeanBackward0>) tensor(-4562.9697, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0023, grad_fn=<MeanBackward0>) tensor(2411.2844, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-2286.7273, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(0.0041, grad_fn=<MeanBackward0>) tensor(4208.0649, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-2277.0896, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0010, grad_fn=<MeanBackward0>) tensor(-1015.3568, grad_fn=<SumBackward0>) 1011\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(1407.3484, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0031, grad_fn=<MeanBackward0>) tensor(3324.1533, grad_fn=<SumBackward0>) 1043\n",
      "total -21.0\n",
      "loss tensor(-0.0044, grad_fn=<MeanBackward0>) tensor(-4451.4248, grad_fn=<SumBackward0>) 1007\n",
      "total -21.0\n",
      "loss tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(-668.8209, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(0.0031, grad_fn=<MeanBackward0>) tensor(3250.6565, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(-1985.4957, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0018, grad_fn=<MeanBackward0>) tensor(1838.6697, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(-0.0048, grad_fn=<MeanBackward0>) tensor(-5079.9873, grad_fn=<SumBackward0>) 1029\n",
      "total -21.0\n",
      "loss tensor(-0.0009, grad_fn=<MeanBackward0>) tensor(-962.5842, grad_fn=<SumBackward0>) 1030\n",
      "total -21.0\n",
      "loss tensor(0.0015, grad_fn=<MeanBackward0>) tensor(1588.8550, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(0.0026, grad_fn=<MeanBackward0>) tensor(2681.5173, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(0.0037, grad_fn=<MeanBackward0>) tensor(3799.7717, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(0.0040, grad_fn=<MeanBackward0>) tensor(4256.0967, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(-634.5312, grad_fn=<SumBackward0>) 1003\n",
      "total -21.0\n",
      "loss tensor(-0.0039, grad_fn=<MeanBackward0>) tensor(-4031.2129, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(-0.0068, grad_fn=<MeanBackward0>) tensor(-7145.3833, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0022, grad_fn=<MeanBackward0>) tensor(2319.7183, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0016, grad_fn=<MeanBackward0>) tensor(1637.8053, grad_fn=<SumBackward0>) 1014\n",
      "total -21.0\n",
      "loss tensor(0.0001, grad_fn=<MeanBackward0>) tensor(103.3514, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(0.0023, grad_fn=<MeanBackward0>) tensor(2376.3704, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-708.2296, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(0.0003, grad_fn=<MeanBackward0>) tensor(287.2477, grad_fn=<SumBackward0>) 1011\n",
      "total -21.0\n",
      "loss tensor(-0.0012, grad_fn=<MeanBackward0>) tensor(-1271.4760, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0036, grad_fn=<MeanBackward0>) tensor(3745.2876, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(-0.0052, grad_fn=<MeanBackward0>) tensor(-5427.0659, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0006, grad_fn=<MeanBackward0>) tensor(594.5614, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(-404.7758, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0065, grad_fn=<MeanBackward0>) tensor(-6750.5728, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(-0.0027, grad_fn=<MeanBackward0>) tensor(-2827.0894, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-679.1588, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0012, grad_fn=<MeanBackward0>) tensor(1235.8613, grad_fn=<SumBackward0>) 1022\n",
      "total -21.0\n",
      "loss tensor(0.0008, grad_fn=<MeanBackward0>) tensor(846.6976, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(-0.0035, grad_fn=<MeanBackward0>) tensor(-3665.7893, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(-0.0028, grad_fn=<MeanBackward0>) tensor(-2908.7747, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(0.0051, grad_fn=<MeanBackward0>) tensor(5478.9541, grad_fn=<SumBackward0>) 1034\n",
      "total -21.0\n",
      "loss tensor(0.0028, grad_fn=<MeanBackward0>) tensor(2934.3096, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(0.0012, grad_fn=<MeanBackward0>) tensor(1266.3198, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(0.0050, grad_fn=<MeanBackward0>) tensor(5348.2139, grad_fn=<SumBackward0>) 1036\n",
      "total -21.0\n",
      "loss tensor(8.8848e-05, grad_fn=<MeanBackward0>) tensor(92.9822, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0050, grad_fn=<MeanBackward0>) tensor(-5205.1709, grad_fn=<SumBackward0>) 1025\n",
      "total -21.0\n",
      "loss tensor(0.0074, grad_fn=<MeanBackward0>) tensor(7656.1284, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(-0.0038, grad_fn=<MeanBackward0>) tensor(-3860.6860, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(0.0025, grad_fn=<MeanBackward0>) tensor(2631.3257, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(0.0027, grad_fn=<MeanBackward0>) tensor(2777.8391, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(0.0037, grad_fn=<MeanBackward0>) tensor(3902.1201, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(0.0067, grad_fn=<MeanBackward0>) tensor(7031.1958, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0042, grad_fn=<MeanBackward0>) tensor(4283.3550, grad_fn=<SumBackward0>) 1007\n",
      "total -21.0\n",
      "loss tensor(-0.0042, grad_fn=<MeanBackward0>) tensor(-4476.4580, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(0.0041, grad_fn=<MeanBackward0>) tensor(4375.0254, grad_fn=<SumBackward0>) 1034\n",
      "total -21.0\n",
      "loss tensor(0.0022, grad_fn=<MeanBackward0>) tensor(2302.5310, grad_fn=<SumBackward0>) 1017\n",
      "total -21.0\n",
      "loss tensor(0.0016, grad_fn=<MeanBackward0>) tensor(1627.2106, grad_fn=<SumBackward0>) 1019\n",
      "total -21.0\n",
      "loss tensor(-0.0035, grad_fn=<MeanBackward0>) tensor(-3525.9043, grad_fn=<SumBackward0>) 1006\n",
      "total -21.0\n",
      "loss tensor(0.0018, grad_fn=<MeanBackward0>) tensor(1934.4690, grad_fn=<SumBackward0>) 1030\n",
      "total -21.0\n",
      "loss tensor(0.0067, grad_fn=<MeanBackward0>) tensor(7174.6318, grad_fn=<SumBackward0>) 1034\n",
      "total -21.0\n",
      "loss tensor(0.0010, grad_fn=<MeanBackward0>) tensor(1030.6083, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(-0.0043, grad_fn=<MeanBackward0>) tensor(-4460.3848, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(9.6269e-05, grad_fn=<MeanBackward0>) tensor(101.5382, grad_fn=<SumBackward0>) 1027\n",
      "total -21.0\n",
      "loss tensor(0.0005, grad_fn=<MeanBackward0>) tensor(571.7002, grad_fn=<SumBackward0>) 1029\n",
      "total -21.0\n",
      "loss tensor(0.0056, grad_fn=<MeanBackward0>) tensor(5628.9453, grad_fn=<SumBackward0>) 1006\n",
      "total -21.0\n",
      "loss tensor(-0.0003, grad_fn=<MeanBackward0>) tensor(-276.9620, grad_fn=<SumBackward0>) 1021\n",
      "total -21.0\n",
      "loss tensor(-0.0021, grad_fn=<MeanBackward0>) tensor(-2265.5554, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0011, grad_fn=<MeanBackward0>) tensor(1109.7681, grad_fn=<SumBackward0>) 1020\n",
      "total -21.0\n",
      "loss tensor(0.0027, grad_fn=<MeanBackward0>) tensor(2858.2505, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(0.0016, grad_fn=<MeanBackward0>) tensor(1622.8193, grad_fn=<SumBackward0>) 1015\n",
      "total -21.0\n",
      "loss tensor(0.0002, grad_fn=<MeanBackward0>) tensor(216.2751, grad_fn=<SumBackward0>) 1028\n",
      "total -21.0\n",
      "loss tensor(0.0042, grad_fn=<MeanBackward0>) tensor(4410.2017, grad_fn=<SumBackward0>) 1026\n",
      "total -21.0\n",
      "loss tensor(-0.0024, grad_fn=<MeanBackward0>) tensor(-2887.0996, grad_fn=<SumBackward0>) 1097\n",
      "total -21.0\n",
      "loss tensor(-0.0017, grad_fn=<MeanBackward0>) tensor(-1768.7079, grad_fn=<SumBackward0>) 1013\n",
      "total -21.0\n",
      "loss tensor(-0.0007, grad_fn=<MeanBackward0>) tensor(-882.1246, grad_fn=<SumBackward0>) 1087\n",
      "total -21.0\n",
      "loss tensor(0.0021, grad_fn=<MeanBackward0>) tensor(2529.6086, grad_fn=<SumBackward0>) 1086\n",
      "total -21.0\n",
      "loss tensor(0.0010, grad_fn=<MeanBackward0>) tensor(1029.3280, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0030, grad_fn=<MeanBackward0>) tensor(-3063.3997, grad_fn=<SumBackward0>) 1010\n",
      "total -21.0\n",
      "loss tensor(0.0009, grad_fn=<MeanBackward0>) tensor(953.4843, grad_fn=<SumBackward0>) 1011\n",
      "total -21.0\n",
      "loss tensor(-0.0013, grad_fn=<MeanBackward0>) tensor(-1812.9777, grad_fn=<SumBackward0>) 1184\n",
      "total -21.0\n",
      "loss tensor(-0.0002, grad_fn=<MeanBackward0>) tensor(-214.0206, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(4.8239e-05, grad_fn=<MeanBackward0>) tensor(58.6881, grad_fn=<SumBackward0>) 1103\n",
      "total -21.0\n",
      "loss tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(-376.0297, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(0.0014, grad_fn=<MeanBackward0>) tensor(1461.6984, grad_fn=<SumBackward0>) 1018\n",
      "total -21.0\n",
      "loss tensor(-0.0005, grad_fn=<MeanBackward0>) tensor(-485.2563, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(-0.0022, grad_fn=<MeanBackward0>) tensor(-3534.6919, grad_fn=<SumBackward0>) 1258\n",
      "total -21.0\n",
      "loss tensor(0.0018, grad_fn=<MeanBackward0>) tensor(1860.5471, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0002, grad_fn=<MeanBackward0>) tensor(-199.7189, grad_fn=<SumBackward0>) 1012\n",
      "total -21.0\n",
      "loss tensor(-0.0028, grad_fn=<MeanBackward0>) tensor(-2993.3252, grad_fn=<SumBackward0>) 1035\n",
      "total -21.0\n",
      "loss tensor(0.0037, grad_fn=<MeanBackward0>) tensor(3807.4543, grad_fn=<SumBackward0>) 1009\n",
      "total -21.0\n",
      "loss tensor(7.3971e-05, grad_fn=<MeanBackward0>) tensor(78.3240, grad_fn=<SumBackward0>) 1029\n",
      "total -21.0\n",
      "loss tensor(0.0004, grad_fn=<MeanBackward0>) tensor(425.7284, grad_fn=<SumBackward0>) 1024\n",
      "total -21.0\n",
      "loss tensor(0.0013, grad_fn=<MeanBackward0>) tensor(1349.6320, grad_fn=<SumBackward0>) 1023\n",
      "total -21.0\n",
      "loss tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(-579.2511, grad_fn=<SumBackward0>) 1180\n",
      "total -21.0\n",
      "loss tensor(-0.0012, grad_fn=<MeanBackward0>) tensor(-2520.8081, grad_fn=<SumBackward0>) 1433\n",
      "total -19.0\n",
      "loss tensor(-0.0004, grad_fn=<MeanBackward0>) tensor(-477.6093, grad_fn=<SumBackward0>) 1095\n",
      "total -21.0\n",
      "loss tensor(0.0021, grad_fn=<MeanBackward0>) tensor(3658.5332, grad_fn=<SumBackward0>) 1311\n",
      "total -20.0\n",
      "loss tensor(-0.0048, grad_fn=<MeanBackward0>) tensor(-10233.2637, grad_fn=<SumBackward0>) 1457\n",
      "total -20.0\n",
      "loss tensor(-0.0015, grad_fn=<MeanBackward0>) tensor(-1823.2754, grad_fn=<SumBackward0>) 1092\n",
      "total -21.0\n",
      "loss tensor(-0.0001, grad_fn=<MeanBackward0>) tensor(-201.6516, grad_fn=<SumBackward0>) 1196\n",
      "total -21.0\n",
      "loss tensor(-0.0005, grad_fn=<MeanBackward0>) tensor(-606.3372, grad_fn=<SumBackward0>) 1105\n",
      "total -21.0\n",
      "loss tensor(0.0012, grad_fn=<MeanBackward0>) tensor(1469.9922, grad_fn=<SumBackward0>) 1105\n",
      "total -21.0\n",
      "loss tensor(-0.0009, grad_fn=<MeanBackward0>) tensor(-1060.5951, grad_fn=<SumBackward0>) 1097\n",
      "total -21.0\n",
      "loss tensor(-0.0122, grad_fn=<MeanBackward0>) tensor(-19436.9199, grad_fn=<SumBackward0>) 1261\n",
      "total -21.0\n",
      "loss tensor(0.0008, grad_fn=<MeanBackward0>) tensor(1322.4346, grad_fn=<SumBackward0>) 1255\n",
      "total -21.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[257], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m current_x \u001b[38;5;241m=\u001b[39m prepro(observation)\n\u001b[1;32m      6\u001b[0m gap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(current_x) \u001b[38;5;28;01mif\u001b[39;00m prev_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m current_x \u001b[38;5;241m-\u001b[39m prev_x\n\u001b[0;32m----> 7\u001b[0m action_prob, action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m action \u001b[38;5;66;03m### 2,3\u001b[39;00m\n\u001b[1;32m      9\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(action)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 更新为新的返回值\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[236], line 25\u001b[0m, in \u001b[0;36mActor.sample_action\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample_action\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 25\u001b[0m         prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#             prob = torch.clamp(prob, min=1e-8, max=1-1e-8)  # 关键修复\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#             m = Bernoulli(prob)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#             action = m.sample().item()  # 1 表示向上，0 表示向下\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#             return int(action) + 2\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[236], line 18\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "observation, _ = env.reset()\n",
    "total_sum = 0\n",
    "prev_x = None\n",
    "while True:\n",
    "    current_x = prepro(observation)\n",
    "    gap = np.zeros_like(current_x) if prev_x is None else current_x - prev_x\n",
    "    action_prob, action = agent.actor.sample_action(torch.from_numpy(gap))\n",
    "    y = action ### 2,3\n",
    "    observation, reward, terminated, truncated, info = env.step(int(action)+2)  # 更新为新的返回值\n",
    "    prev_x = current_x\n",
    "    total_sum += reward\n",
    "    agent.memory.push(gap,action,reward,terminated,float(y),action_prob)\n",
    "    if terminated or truncated:\n",
    "        agent.update()\n",
    "        print('total',total_sum)\n",
    "        observation, _ = env.reset()\n",
    "        total_sum = 0\n",
    "        prev_x = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in agent.actor.parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b355be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pong-v4\",render_mode=\"human\")\n",
    "observation, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0ca71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(D,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "888ea406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 19:22:21.302 python[26815:139541057] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-12 19:22:21.302 python[26815:139541057] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "prev_x = None\n",
    "while True:\n",
    "    current_x = prepro(observation)\n",
    "    gap = np.zeros_like(current_x) if prev_x is None else current_x - prev_x\n",
    "    prob = model(torch.from_numpy(gap))\n",
    "    action = 2 if prob < 0.5 else 3\n",
    "    observation, reward, terminated, truncated, info = env.step(action)  # 更新为新的返回值\n",
    "    prev_x = current_x\n",
    "    if terminated:\n",
    "        observation, _ = env.reset()\n",
    "        prev_x = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813aaf1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
