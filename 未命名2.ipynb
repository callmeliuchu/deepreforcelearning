{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5b4cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_tensor tensor([[1., 2., 3., 4.]])\n",
      "probs tensor([[0.6368, 0.3632]], grad_fn=<SoftmaxBackward0>)\n",
      "dist Categorical(probs: torch.Size([1, 2]))\n",
      "action tensor([0])\n",
      "log_probs tensor([-0.4513])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单的 Actor 模型\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Linear(4, 2)  # 假设输入维度为4，输出维度为2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "# 假设的输入状态\n",
    "state = [1.0, 2.0, 3.0, 4.0]\n",
    "device = 'cpu'\n",
    "\n",
    "# 初始化模型\n",
    "actor = Actor().to(device)\n",
    "\n",
    "# 转换状态并通过模型获取概率分布\n",
    "state_tensor = torch.tensor(state, device=device, dtype=torch.float32).unsqueeze(dim=0)\n",
    "print('state_tensor',state_tensor)\n",
    "probs = actor(state_tensor)\n",
    "print('probs',probs)\n",
    "\n",
    "# 创建分布并采样动作\n",
    "dist = Categorical(probs)\n",
    "print('dist',dist)\n",
    "action = dist.sample()\n",
    "print('action',action)\n",
    "\n",
    "# 获取动作的对数概率\n",
    "log_probs = dist.log_prob(action).detach()\n",
    "print('log_probs',log_probs)\n",
    "# 输出动作\n",
    "action_value = action.detach().cpu().numpy().item()\n",
    "print(action_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9af6fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a3967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim,output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return F.softmax(logits,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f912bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim,output_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981b98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.bool8 = np.bool_\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac9d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")  # 或 \"rgb_array\"\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "state,_ = env.reset()\n",
    "next_state, reward, terminated, truncated, _ = env.step(0)  # 注意这里的返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99474edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02936008, -0.04976663, -0.01232278, -0.02797908], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa0e7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249e2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayQue:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.buffer = deque()\n",
    "    \n",
    "    def sample(self):\n",
    "        return zip(*self.buffer)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def push(self,transitions):\n",
    "        self.buffer.append(transitions)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0e0d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.critic = Critic(n_states,1,256)\n",
    "        self.actor = Actor(n_states,n_actions,256)\n",
    "        self.memory = ReplayQue()\n",
    "        self.actor_optimizer = Adam(self.actor.parameters())\n",
    "        self.critic_optimizer = Adam(self.critic.parameters())\n",
    "    \n",
    "    def sample_action(self,state):\n",
    "        state = torch.tensor(state)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action).detach()\n",
    "        return action.detach().cpu().numpy().item(), logprob\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self,state):\n",
    "        state = torch.tensor(state)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.detach().cpu().numpy().item()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.memory) % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        \n",
    "#         next_state, reward, terminated\n",
    "        old_states,old_logprobs,old_rewards,old_dones,old_actions = self.memory.sample()\n",
    "        old_states = torch.tensor(old_states)\n",
    "        old_logprobs = torch.tensor(old_logprobs)\n",
    "        old_actions = torch.tensor(old_actions)\n",
    "        \n",
    "        \n",
    "        returns = []\n",
    "        discount_sum = 0\n",
    "        for reward,done in zip(reversed(old_rewards),reversed(old_dones)):\n",
    "            if done:\n",
    "                discount_sum = 0\n",
    "            discount_sum = discount_sum * 0.99 + reward\n",
    "            returns.insert(0,discount_sum)\n",
    "        \n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std()+1e-5)\n",
    "        \n",
    "        \n",
    "        for _ in range(4):\n",
    "            values = self.critic(old_states)\n",
    "            advantage = returns - values.detach()\n",
    "            probs = self.actor(old_states)\n",
    "            dist = Categorical(probs)\n",
    "            logprobs = dist.log_prob(old_actions)\n",
    "#             print(logprobs.shape,old_logprobs.shape)\n",
    "            ratio = torch.exp(logprobs - old_logprobs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio,0.8,1.2) * advantage\n",
    "            actor_loss = -torch.min(surr1,surr2).mean() + dist.entropy().mean() * 0.01\n",
    "            critic_loss = ((returns - values)**2).mean()\n",
    "            \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            \n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            \n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "        \n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aadeb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,agent):\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(10000):\n",
    "        \n",
    "        state,_ = env.reset()\n",
    "        train_reward = 0\n",
    "        for _ in range(1000):\n",
    "            action,logprob = agent.sample_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # 注意这里的返回值\n",
    "    #         old_states,old_logprobs,old_rewards,old_dones,old_actions\n",
    "            agent.memory.push((state,logprob,reward,terminated,action))\n",
    "            agent.update()\n",
    "            state = next_state\n",
    "            \n",
    "            train_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "#         print('train reward.....', train_reward)\n",
    "        #### eval the reward\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            state,_ = env.reset()\n",
    "            eval_reward = 0\n",
    "            for _ in range(1000):\n",
    "                action = agent.predict(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)  # 注意这里的返回值\n",
    "                state = next_state\n",
    "                eval_reward += reward\n",
    "                if terminated:\n",
    "                    break\n",
    "            print('eval reward.....', eval_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e34d07fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z2/kds62tbj3x93zghv2_jz1xsr0000gn/T/ipykernel_97966/222536942.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  old_states = torch.tensor(old_states)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval reward..... 99.0\n",
      "eval reward..... 926.0\n",
      "eval reward..... 715.0\n",
      "eval reward..... 135.0\n",
      "eval reward..... 121.0\n",
      "eval reward..... 152.0\n",
      "eval reward..... 109.0\n",
      "eval reward..... 103.0\n",
      "eval reward..... 123.0\n",
      "eval reward..... 103.0\n",
      "eval reward..... 504.0\n",
      "eval reward..... 213.0\n",
      "eval reward..... 102.0\n",
      "eval reward..... 208.0\n",
      "eval reward..... 350.0\n",
      "eval reward..... 199.0\n",
      "eval reward..... 77.0\n",
      "eval reward..... 205.0\n",
      "eval reward..... 112.0\n",
      "eval reward..... 63.0\n",
      "eval reward..... 78.0\n",
      "eval reward..... 119.0\n",
      "eval reward..... 99.0\n",
      "eval reward..... 120.0\n",
      "eval reward..... 112.0\n",
      "eval reward..... 118.0\n",
      "eval reward..... 141.0\n",
      "eval reward..... 118.0\n",
      "eval reward..... 179.0\n",
      "eval reward..... 288.0\n",
      "eval reward..... 400.0\n",
      "eval reward..... 78.0\n",
      "eval reward..... 61.0\n",
      "eval reward..... 111.0\n",
      "eval reward..... 79.0\n",
      "eval reward..... 98.0\n",
      "eval reward..... 198.0\n",
      "eval reward..... 406.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 184.0\n",
      "eval reward..... 196.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 334.0\n",
      "eval reward..... 139.0\n",
      "eval reward..... 93.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 133.0\n",
      "eval reward..... 98.0\n",
      "eval reward..... 136.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 733.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 780.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 705.0\n",
      "eval reward..... 346.0\n",
      "eval reward..... 362.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 363.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 323.0\n",
      "eval reward..... 247.0\n",
      "eval reward..... 166.0\n",
      "eval reward..... 299.0\n",
      "eval reward..... 372.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 763.0\n",
      "eval reward..... 510.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 803.0\n",
      "eval reward..... 534.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 366.0\n",
      "eval reward..... 246.0\n",
      "eval reward..... 214.0\n",
      "eval reward..... 574.0\n",
      "eval reward..... 449.0\n",
      "eval reward..... 473.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 369.0\n",
      "eval reward..... 393.0\n",
      "eval reward..... 601.0\n",
      "eval reward..... 1000.0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "train(env,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e0a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval reward..... 667.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 361.0\n",
      "eval reward..... 361.0\n",
      "eval reward..... 508.0\n",
      "eval reward..... 392.0\n",
      "eval reward..... 458.0\n",
      "eval reward..... 576.0\n",
      "eval reward..... 343.0\n",
      "eval reward..... 307.0\n",
      "eval reward..... 257.0\n",
      "eval reward..... 253.0\n",
      "eval reward..... 196.0\n",
      "eval reward..... 196.0\n",
      "eval reward..... 154.0\n",
      "eval reward..... 188.0\n",
      "eval reward..... 177.0\n",
      "eval reward..... 138.0\n",
      "eval reward..... 140.0\n",
      "eval reward..... 138.0\n",
      "eval reward..... 135.0\n",
      "eval reward..... 151.0\n",
      "eval reward..... 171.0\n",
      "eval reward..... 147.0\n",
      "eval reward..... 133.0\n",
      "eval reward..... 170.0\n",
      "eval reward..... 152.0\n",
      "eval reward..... 200.0\n",
      "eval reward..... 175.0\n",
      "eval reward..... 179.0\n",
      "eval reward..... 167.0\n",
      "eval reward..... 185.0\n",
      "eval reward..... 146.0\n",
      "eval reward..... 156.0\n",
      "eval reward..... 158.0\n",
      "eval reward..... 188.0\n",
      "eval reward..... 166.0\n",
      "eval reward..... 134.0\n",
      "eval reward..... 170.0\n",
      "eval reward..... 142.0\n",
      "eval reward..... 154.0\n",
      "eval reward..... 170.0\n",
      "eval reward..... 171.0\n",
      "eval reward..... 173.0\n",
      "eval reward..... 153.0\n",
      "eval reward..... 187.0\n",
      "eval reward..... 184.0\n",
      "eval reward..... 229.0\n",
      "eval reward..... 287.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 603.0\n",
      "eval reward..... 633.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 715.0\n",
      "eval reward..... 989.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 696.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 478.0\n",
      "eval reward..... 715.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 822.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 496.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 863.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 968.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 655.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 807.0\n",
      "eval reward..... 1000.0\n",
      "eval reward..... 580.0\n",
      "eval reward..... 986.0\n",
      "eval reward..... 280.0\n",
      "eval reward..... 345.0\n",
      "eval reward..... 274.0\n",
      "eval reward..... 373.0\n",
      "eval reward..... 607.0\n",
      "eval reward..... 288.0\n"
     ]
    }
   ],
   "source": [
    "train(env,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d35b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"human\")  # 或 \"rgb_array\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34fa4242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 09:15:04.864 python[97966:67564696] _TIPropertyValueIsValid called with 16 on nil context!\n",
      "2025-03-13 09:15:04.864 python[97966:67564696] imkxpc_getApplicationProperty:reply: called with incorrect property value 16, bailing.\n",
      "2025-03-13 09:15:04.864 python[97966:67564696] Text input context does not respond to _valueForTIProperty:\n"
     ]
    }
   ],
   "source": [
    "state,_ = env.reset()\n",
    "eval_reward = 0\n",
    "for _ in range(1000):\n",
    "    action = agent.predict(state)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)  # 注意这里的返回值\n",
    "    env.render()\n",
    "    state = next_state\n",
    "    eval_reward += reward\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2f24f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 09:15:22.097 python[97966:67564696] _TIPropertyValueIsValid called with 16 on nil context!\n",
      "2025-03-13 09:15:22.097 python[97966:67564696] imkxpc_getApplicationProperty:reply: called with incorrect property value 16, bailing.\n",
      "2025-03-13 09:15:22.097 python[97966:67564696] Text input context does not respond to _valueForTIProperty:\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fefa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa54865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
