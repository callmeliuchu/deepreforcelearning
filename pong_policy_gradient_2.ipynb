{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18f391aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "61504f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建环境\n",
    "env = gym.make(\"Pong-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "299a2edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# state,_ = env.reset()\n",
    "# done = False\n",
    "# count = 0\n",
    "# while not done:\n",
    "# #     env.render()\n",
    "#     action = int(np.random.choice([2,3]))\n",
    "#     next_state, reward, done, truncated, _ = env.step(action)\n",
    "#     print(action,reward)\n",
    "#     count += 1\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d5bc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f5e00ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f5b4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\"将 210x160x3 uint8 帧预处理为 6400 (80x80) 1D float 向量\"\"\"\n",
    "    I = I[35:195]  # 裁剪\n",
    "    I = I[::2, ::2, 0]  # 下采样因子为 2\n",
    "    I[I == 144] = 0  # 删除背景类型 1\n",
    "    I[I == 109] = 0  # 删除背景类型 2\n",
    "    I[I != 0] = 1  # 其他设置为 1\n",
    "    return I.astype(np.float32).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5024f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim,200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(200,output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,state):\n",
    "        ### n\n",
    "        x = self.linear1(state)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x) # n\n",
    "        x = self.softmax(x) # n\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52feccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "np.bool8 = np.bool_\n",
    "\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cada96ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.policy_net = PolicyNet(6400,2)\n",
    "        self.optimizer = AdamW(self.policy_net.parameters(),lr=1e-3)\n",
    "    \n",
    "    def sample_action(self,state):\n",
    "        probs = self.policy_net(state) # 4\n",
    "        if np.random.uniform() < 0.2:\n",
    "            action = np.random.randint(0,2)\n",
    "            return action + 2, torch.log(probs[action]+1e-8)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item()+2,log_prob\n",
    "    \n",
    "    def update(self,rewards,log_probs):\n",
    "        ### 一次游戏时间\n",
    "        ret = []\n",
    "        adding = 0\n",
    "        for r in rewards[::-1]:\n",
    "            if r != 0:\n",
    "                adding = 0\n",
    "            adding = adding * 0.99 + r\n",
    "            ret.insert(0,adding)\n",
    "        ret = torch.FloatTensor(ret)\n",
    "        ret = ret - ret.mean()\n",
    "        ret = ret / (ret.std()+1e-8)\n",
    "        \n",
    "        r_log_probs = []\n",
    "        for r,log_prob in zip(ret,log_probs):\n",
    "            r_log_probs.append(-r*log_prob)\n",
    "        r_log_probs = torch.vstack(r_log_probs)\n",
    "        \n",
    "        loss = r_log_probs.sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184d817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36c762a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env):\n",
    "    success_count = []\n",
    "    max_size = 2000\n",
    "    for epoch in range(20000):\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        terminated = False\n",
    "        state,_ = env.reset()\n",
    "        prev_x = None\n",
    "        while not terminated:\n",
    "            x = prepro(state)\n",
    "            diff = np.zeros(6400) if prev_x is None else x - prev_x\n",
    "            prev_x = x\n",
    "            diff = torch.FloatTensor(diff)\n",
    "            action, log_prob = agent.sample_action(diff)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "        \n",
    "        loss = agent.update(rewards,log_probs) \n",
    "        \n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "#             torch.save('pong.pt',agent.policy_net)\n",
    "            torch.save(agent.policy_net,'pong.pt')\n",
    "            print(f'epoch: {epoch}, loss: {loss}, rewards: {sum(rewards)}, count: {len(rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "57d0b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b264415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(agent.policy_net,'pong.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8e722c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (linear1): Linear(in_features=6400, out_features=200, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=200, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.load('pong.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7bd2bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 100.39936828613281, rewards: -19.0, count: 1851\n",
      "epoch: 19, loss: 99.27107238769531, rewards: -21.0, count: 2740\n",
      "epoch: 29, loss: 97.95178985595703, rewards: -19.0, count: 2083\n",
      "epoch: 39, loss: 81.42912292480469, rewards: -20.0, count: 1716\n",
      "epoch: 49, loss: 80.0657958984375, rewards: -19.0, count: 1842\n",
      "epoch: 59, loss: 69.82518005371094, rewards: -20.0, count: 2199\n",
      "epoch: 69, loss: 80.46188354492188, rewards: -21.0, count: 1502\n",
      "epoch: 79, loss: 81.82455444335938, rewards: -17.0, count: 2011\n",
      "epoch: 89, loss: 46.32903289794922, rewards: -20.0, count: 2446\n",
      "epoch: 99, loss: 112.66027069091797, rewards: -19.0, count: 2011\n",
      "epoch: 109, loss: 110.7575912475586, rewards: -17.0, count: 2094\n",
      "epoch: 119, loss: 63.838226318359375, rewards: -20.0, count: 2045\n",
      "epoch: 129, loss: 43.82166290283203, rewards: -20.0, count: 1410\n",
      "epoch: 139, loss: 73.86117553710938, rewards: -18.0, count: 2363\n",
      "epoch: 149, loss: 42.780494689941406, rewards: -20.0, count: 1806\n",
      "epoch: 159, loss: 78.47926330566406, rewards: -21.0, count: 1338\n",
      "epoch: 169, loss: 60.56501007080078, rewards: -19.0, count: 1905\n",
      "epoch: 179, loss: 54.38261413574219, rewards: -20.0, count: 1325\n",
      "epoch: 189, loss: 80.81228637695312, rewards: -19.0, count: 1942\n",
      "epoch: 199, loss: 70.22476959228516, rewards: -20.0, count: 2548\n",
      "epoch: 209, loss: 46.56510925292969, rewards: -20.0, count: 2198\n",
      "epoch: 219, loss: 103.63349914550781, rewards: -17.0, count: 1918\n",
      "epoch: 229, loss: 79.56826782226562, rewards: -19.0, count: 2067\n",
      "epoch: 239, loss: 41.98748016357422, rewards: -21.0, count: 2244\n",
      "epoch: 249, loss: 91.974365234375, rewards: -15.0, count: 2353\n",
      "epoch: 259, loss: 111.71358489990234, rewards: -21.0, count: 2309\n",
      "epoch: 269, loss: 47.23591613769531, rewards: -20.0, count: 1697\n",
      "epoch: 279, loss: 71.02275085449219, rewards: -20.0, count: 2026\n",
      "epoch: 289, loss: 77.32009887695312, rewards: -15.0, count: 2434\n",
      "epoch: 299, loss: 56.86884307861328, rewards: -19.0, count: 2876\n",
      "epoch: 309, loss: 75.07382202148438, rewards: -21.0, count: 1838\n",
      "epoch: 319, loss: 78.67269897460938, rewards: -18.0, count: 2695\n",
      "epoch: 329, loss: 78.01350402832031, rewards: -21.0, count: 2062\n",
      "epoch: 339, loss: 45.62126922607422, rewards: -19.0, count: 2473\n",
      "epoch: 349, loss: 35.749839782714844, rewards: -18.0, count: 2293\n",
      "epoch: 359, loss: 68.7810287475586, rewards: -21.0, count: 2002\n",
      "epoch: 369, loss: 71.83870697021484, rewards: -18.0, count: 3022\n",
      "epoch: 379, loss: 43.91830062866211, rewards: -18.0, count: 1961\n",
      "epoch: 389, loss: 13.00267505645752, rewards: -18.0, count: 1813\n",
      "epoch: 399, loss: 61.43313217163086, rewards: -18.0, count: 1723\n",
      "epoch: 409, loss: 14.724308013916016, rewards: -17.0, count: 2435\n",
      "epoch: 419, loss: 44.7635612487793, rewards: -19.0, count: 2181\n",
      "epoch: 429, loss: 38.566673278808594, rewards: -20.0, count: 1851\n",
      "epoch: 439, loss: 18.72978401184082, rewards: -18.0, count: 1735\n",
      "epoch: 449, loss: 66.21394348144531, rewards: -19.0, count: 2153\n",
      "epoch: 459, loss: 48.031646728515625, rewards: -19.0, count: 1996\n",
      "epoch: 469, loss: 77.79621887207031, rewards: -21.0, count: 2576\n",
      "epoch: 479, loss: 21.210437774658203, rewards: -17.0, count: 2721\n",
      "epoch: 489, loss: 41.64886474609375, rewards: -16.0, count: 2784\n",
      "epoch: 499, loss: 32.560489654541016, rewards: -20.0, count: 1453\n",
      "epoch: 509, loss: 57.30976104736328, rewards: -18.0, count: 2301\n",
      "epoch: 519, loss: 42.73479461669922, rewards: -19.0, count: 2111\n",
      "epoch: 529, loss: 25.607254028320312, rewards: -16.0, count: 2223\n",
      "epoch: 539, loss: 46.543701171875, rewards: -17.0, count: 2749\n",
      "epoch: 549, loss: 23.844615936279297, rewards: -19.0, count: 1604\n",
      "epoch: 559, loss: 51.0955924987793, rewards: -19.0, count: 1595\n",
      "epoch: 569, loss: 25.612695693969727, rewards: -20.0, count: 2112\n",
      "epoch: 579, loss: 37.28253173828125, rewards: -18.0, count: 2464\n",
      "epoch: 589, loss: 52.96837615966797, rewards: -19.0, count: 2253\n",
      "epoch: 599, loss: 45.099693298339844, rewards: -19.0, count: 2083\n",
      "epoch: 609, loss: 44.02954864501953, rewards: -20.0, count: 1876\n",
      "epoch: 619, loss: 31.451345443725586, rewards: -20.0, count: 2288\n",
      "epoch: 629, loss: 39.72550582885742, rewards: -19.0, count: 1932\n",
      "epoch: 639, loss: 76.86614990234375, rewards: -16.0, count: 2302\n",
      "epoch: 649, loss: 81.69171142578125, rewards: -20.0, count: 2274\n",
      "epoch: 659, loss: 84.88964080810547, rewards: -20.0, count: 2444\n",
      "epoch: 669, loss: 38.285614013671875, rewards: -20.0, count: 1480\n",
      "epoch: 679, loss: 28.05299949645996, rewards: -19.0, count: 2092\n",
      "epoch: 689, loss: 50.13943862915039, rewards: -18.0, count: 2700\n",
      "epoch: 699, loss: 23.368141174316406, rewards: -17.0, count: 2501\n",
      "epoch: 709, loss: 56.181949615478516, rewards: -20.0, count: 1719\n",
      "epoch: 719, loss: 58.74835205078125, rewards: -19.0, count: 1543\n",
      "epoch: 729, loss: 60.154876708984375, rewards: -16.0, count: 2713\n",
      "epoch: 739, loss: 35.274993896484375, rewards: -19.0, count: 2112\n",
      "epoch: 749, loss: 34.11066818237305, rewards: -18.0, count: 2462\n",
      "epoch: 759, loss: 53.30134963989258, rewards: -20.0, count: 1809\n",
      "epoch: 769, loss: 32.86019515991211, rewards: -19.0, count: 2012\n",
      "epoch: 779, loss: 55.411582946777344, rewards: -18.0, count: 1887\n",
      "epoch: 789, loss: 38.203712463378906, rewards: -20.0, count: 1531\n",
      "epoch: 799, loss: 40.775184631347656, rewards: -19.0, count: 1703\n",
      "epoch: 809, loss: 23.10834503173828, rewards: -19.0, count: 2002\n",
      "epoch: 819, loss: 71.0126953125, rewards: -18.0, count: 2289\n",
      "epoch: 829, loss: 43.68006896972656, rewards: -18.0, count: 2296\n",
      "epoch: 839, loss: 48.171897888183594, rewards: -19.0, count: 2095\n",
      "epoch: 849, loss: 26.899492263793945, rewards: -17.0, count: 2475\n",
      "epoch: 859, loss: 54.51187515258789, rewards: -19.0, count: 2391\n",
      "epoch: 869, loss: 26.025615692138672, rewards: -18.0, count: 1715\n",
      "epoch: 879, loss: 52.18614196777344, rewards: -19.0, count: 2561\n",
      "epoch: 889, loss: 30.326263427734375, rewards: -20.0, count: 1555\n",
      "epoch: 899, loss: 31.26341438293457, rewards: -20.0, count: 1389\n",
      "epoch: 909, loss: 45.90463638305664, rewards: -18.0, count: 1554\n",
      "epoch: 919, loss: 34.098758697509766, rewards: -16.0, count: 2724\n",
      "epoch: 929, loss: 21.639890670776367, rewards: -20.0, count: 1776\n",
      "epoch: 939, loss: 36.73216247558594, rewards: -16.0, count: 2151\n",
      "epoch: 949, loss: 85.48577880859375, rewards: -18.0, count: 1902\n",
      "epoch: 959, loss: 49.33110046386719, rewards: -20.0, count: 2028\n",
      "epoch: 969, loss: 31.739274978637695, rewards: -17.0, count: 2421\n",
      "epoch: 979, loss: 72.18163299560547, rewards: -19.0, count: 2565\n",
      "epoch: 989, loss: 29.919649124145508, rewards: -18.0, count: 2194\n",
      "epoch: 999, loss: 47.78837203979492, rewards: -17.0, count: 2649\n",
      "epoch: 1009, loss: 33.585693359375, rewards: -18.0, count: 2598\n",
      "epoch: 1019, loss: 57.348697662353516, rewards: -19.0, count: 2566\n",
      "epoch: 1029, loss: 52.2845573425293, rewards: -19.0, count: 1681\n",
      "epoch: 1039, loss: 31.952123641967773, rewards: -20.0, count: 1790\n",
      "epoch: 1049, loss: 18.69768524169922, rewards: -17.0, count: 1608\n",
      "epoch: 1059, loss: 24.66989517211914, rewards: -18.0, count: 1888\n",
      "epoch: 1069, loss: 7.918241024017334, rewards: -19.0, count: 1994\n",
      "epoch: 1079, loss: 30.69409942626953, rewards: -19.0, count: 1839\n",
      "epoch: 1089, loss: 56.28761672973633, rewards: -20.0, count: 2431\n",
      "epoch: 1099, loss: 56.2027702331543, rewards: -17.0, count: 2069\n",
      "epoch: 1109, loss: 3.284763813018799, rewards: -17.0, count: 1994\n",
      "epoch: 1119, loss: 62.775047302246094, rewards: -20.0, count: 2147\n",
      "epoch: 1129, loss: 45.0404052734375, rewards: -18.0, count: 1863\n",
      "epoch: 1139, loss: 3.446103572845459, rewards: -16.0, count: 3189\n",
      "epoch: 1149, loss: 32.16709518432617, rewards: -19.0, count: 1413\n",
      "epoch: 1159, loss: 70.91219329833984, rewards: -17.0, count: 2577\n",
      "epoch: 1169, loss: 19.797895431518555, rewards: -19.0, count: 1983\n",
      "epoch: 1179, loss: 36.68017578125, rewards: -19.0, count: 2328\n",
      "epoch: 1189, loss: 38.044532775878906, rewards: -17.0, count: 2263\n",
      "epoch: 1199, loss: 31.75948715209961, rewards: -19.0, count: 1767\n",
      "epoch: 1209, loss: 44.10454559326172, rewards: -18.0, count: 2599\n",
      "epoch: 1219, loss: 20.990888595581055, rewards: -18.0, count: 3020\n",
      "epoch: 1229, loss: 40.12855911254883, rewards: -20.0, count: 1972\n",
      "epoch: 1239, loss: 51.62737274169922, rewards: -20.0, count: 2022\n",
      "epoch: 1249, loss: 31.470340728759766, rewards: -20.0, count: 1785\n",
      "epoch: 1259, loss: 85.33171844482422, rewards: -21.0, count: 1614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1269, loss: 61.92820739746094, rewards: -19.0, count: 3370\n",
      "epoch: 1279, loss: 63.04595947265625, rewards: -21.0, count: 1747\n",
      "epoch: 1289, loss: 20.298131942749023, rewards: -20.0, count: 2264\n",
      "epoch: 1299, loss: 62.66165542602539, rewards: -20.0, count: 1881\n",
      "epoch: 1309, loss: 49.06715393066406, rewards: -18.0, count: 2439\n",
      "epoch: 1319, loss: 27.15872573852539, rewards: -18.0, count: 2433\n",
      "epoch: 1329, loss: 71.82215118408203, rewards: -20.0, count: 2925\n",
      "epoch: 1339, loss: 26.968229293823242, rewards: -18.0, count: 2211\n",
      "epoch: 1349, loss: 22.21343231201172, rewards: -20.0, count: 2035\n",
      "epoch: 1359, loss: 18.843164443969727, rewards: -20.0, count: 2026\n",
      "epoch: 1369, loss: 47.34242248535156, rewards: -18.0, count: 2355\n",
      "epoch: 1379, loss: 23.19776725769043, rewards: -19.0, count: 2086\n",
      "epoch: 1389, loss: 61.0006103515625, rewards: -18.0, count: 2916\n",
      "epoch: 1399, loss: 28.574010848999023, rewards: -18.0, count: 2059\n",
      "epoch: 1409, loss: 14.454166412353516, rewards: -18.0, count: 2778\n",
      "epoch: 1419, loss: 20.284626007080078, rewards: -18.0, count: 2380\n",
      "epoch: 1429, loss: 32.36408615112305, rewards: -16.0, count: 2600\n",
      "epoch: 1439, loss: 13.21733570098877, rewards: -20.0, count: 1377\n",
      "epoch: 1449, loss: 41.801387786865234, rewards: -16.0, count: 1889\n",
      "epoch: 1459, loss: 15.428791999816895, rewards: -20.0, count: 1800\n",
      "epoch: 1469, loss: 19.803951263427734, rewards: -18.0, count: 1961\n",
      "epoch: 1479, loss: 50.23346710205078, rewards: -20.0, count: 1862\n",
      "epoch: 1489, loss: 32.2291145324707, rewards: -19.0, count: 2082\n",
      "epoch: 1499, loss: 22.03926658630371, rewards: -19.0, count: 1432\n",
      "epoch: 1509, loss: 20.963428497314453, rewards: -18.0, count: 2688\n",
      "epoch: 1519, loss: 10.423576354980469, rewards: -16.0, count: 2314\n",
      "epoch: 1529, loss: 27.639728546142578, rewards: -18.0, count: 2838\n",
      "epoch: 1539, loss: 28.30050277709961, rewards: -21.0, count: 2406\n",
      "epoch: 1549, loss: 18.19042205810547, rewards: -19.0, count: 2010\n",
      "epoch: 1559, loss: 26.017589569091797, rewards: -19.0, count: 1685\n",
      "epoch: 1569, loss: 32.412376403808594, rewards: -21.0, count: 1581\n",
      "epoch: 1579, loss: 34.00680160522461, rewards: -18.0, count: 1917\n",
      "epoch: 1589, loss: 14.002074241638184, rewards: -15.0, count: 2602\n",
      "epoch: 1599, loss: 26.62713050842285, rewards: -20.0, count: 2052\n",
      "epoch: 1609, loss: 38.17388916015625, rewards: -16.0, count: 2606\n",
      "epoch: 1619, loss: 28.888946533203125, rewards: -17.0, count: 2674\n",
      "epoch: 1629, loss: 24.530025482177734, rewards: -18.0, count: 1709\n",
      "epoch: 1639, loss: 36.576839447021484, rewards: -18.0, count: 2228\n",
      "epoch: 1649, loss: 60.04783248901367, rewards: -20.0, count: 2691\n",
      "epoch: 1659, loss: 40.42852020263672, rewards: -13.0, count: 2592\n",
      "epoch: 1669, loss: 37.07041549682617, rewards: -16.0, count: 2295\n",
      "epoch: 1679, loss: 28.30068016052246, rewards: -20.0, count: 1758\n",
      "epoch: 1689, loss: 36.473114013671875, rewards: -20.0, count: 1645\n",
      "epoch: 1699, loss: 39.132423400878906, rewards: -21.0, count: 1968\n",
      "epoch: 1709, loss: 41.53612518310547, rewards: -21.0, count: 1849\n",
      "epoch: 1719, loss: 38.50897979736328, rewards: -20.0, count: 1405\n",
      "epoch: 1729, loss: 48.19212341308594, rewards: -19.0, count: 2569\n",
      "epoch: 1739, loss: 40.82240676879883, rewards: -18.0, count: 1741\n",
      "epoch: 1749, loss: 57.658390045166016, rewards: -19.0, count: 1939\n",
      "epoch: 1759, loss: 41.07114791870117, rewards: -19.0, count: 2561\n",
      "epoch: 1769, loss: 48.445030212402344, rewards: -20.0, count: 1893\n",
      "epoch: 1779, loss: 28.202404022216797, rewards: -18.0, count: 1735\n",
      "epoch: 1789, loss: 47.306270599365234, rewards: -21.0, count: 1677\n",
      "epoch: 1799, loss: 26.457822799682617, rewards: -19.0, count: 2326\n",
      "epoch: 1809, loss: 29.418636322021484, rewards: -19.0, count: 2239\n",
      "epoch: 1819, loss: 34.33216094970703, rewards: -20.0, count: 2016\n",
      "epoch: 1829, loss: 22.911190032958984, rewards: -20.0, count: 1712\n",
      "epoch: 1839, loss: 18.975440979003906, rewards: -16.0, count: 2058\n",
      "epoch: 1849, loss: 5.1557230949401855, rewards: -19.0, count: 2013\n",
      "epoch: 1859, loss: 20.207319259643555, rewards: -17.0, count: 2497\n",
      "epoch: 1869, loss: 38.61054229736328, rewards: -21.0, count: 1483\n",
      "epoch: 1879, loss: 21.492395401000977, rewards: -18.0, count: 2214\n",
      "epoch: 1889, loss: 25.68075180053711, rewards: -18.0, count: 2385\n",
      "epoch: 1899, loss: 52.09001159667969, rewards: -21.0, count: 1737\n",
      "epoch: 1909, loss: -11.564522743225098, rewards: -18.0, count: 1882\n",
      "epoch: 1919, loss: 8.803038597106934, rewards: -18.0, count: 1984\n",
      "epoch: 1929, loss: 50.471290588378906, rewards: -16.0, count: 2944\n",
      "epoch: 1939, loss: 51.790977478027344, rewards: -18.0, count: 2642\n",
      "epoch: 1949, loss: 30.72514533996582, rewards: -18.0, count: 2219\n",
      "epoch: 1959, loss: 31.778026580810547, rewards: -19.0, count: 1838\n",
      "epoch: 1969, loss: 23.518362045288086, rewards: -17.0, count: 2838\n",
      "epoch: 1979, loss: 7.563314437866211, rewards: -18.0, count: 1969\n",
      "epoch: 1989, loss: 4.255891799926758, rewards: -19.0, count: 2390\n",
      "epoch: 1999, loss: 11.207706451416016, rewards: -16.0, count: 2611\n",
      "epoch: 2009, loss: 5.238151550292969, rewards: -19.0, count: 3066\n",
      "epoch: 2019, loss: 8.296051025390625, rewards: -18.0, count: 2138\n",
      "epoch: 2029, loss: 49.10094451904297, rewards: -18.0, count: 2952\n",
      "epoch: 2039, loss: 68.1972427368164, rewards: -18.0, count: 2298\n",
      "epoch: 2049, loss: 11.29733657836914, rewards: -14.0, count: 3046\n",
      "epoch: 2059, loss: 16.610626220703125, rewards: -17.0, count: 2338\n",
      "epoch: 2069, loss: 38.02895736694336, rewards: -18.0, count: 2456\n",
      "epoch: 2079, loss: 15.926019668579102, rewards: -17.0, count: 2993\n",
      "epoch: 2089, loss: 49.844120025634766, rewards: -18.0, count: 2445\n",
      "epoch: 2099, loss: 28.7147216796875, rewards: -19.0, count: 2150\n",
      "epoch: 2109, loss: 4.48152494430542, rewards: -17.0, count: 2002\n",
      "epoch: 2119, loss: 44.97272872924805, rewards: -21.0, count: 2681\n",
      "epoch: 2129, loss: -3.6488473415374756, rewards: -17.0, count: 2398\n",
      "epoch: 2139, loss: 21.657379150390625, rewards: -20.0, count: 2441\n",
      "epoch: 2149, loss: 24.697166442871094, rewards: -17.0, count: 2718\n",
      "epoch: 2159, loss: 8.221109390258789, rewards: -18.0, count: 1641\n",
      "epoch: 2169, loss: 51.96040344238281, rewards: -18.0, count: 2206\n",
      "epoch: 2179, loss: 10.26466178894043, rewards: -16.0, count: 1968\n",
      "epoch: 2189, loss: -0.48253774642944336, rewards: -18.0, count: 2070\n",
      "epoch: 2199, loss: 13.353510856628418, rewards: -18.0, count: 2265\n",
      "epoch: 2209, loss: 19.279373168945312, rewards: -19.0, count: 2621\n",
      "epoch: 2219, loss: 21.153348922729492, rewards: -18.0, count: 2681\n",
      "epoch: 2229, loss: 22.767433166503906, rewards: -20.0, count: 1889\n",
      "epoch: 2239, loss: 16.594385147094727, rewards: -17.0, count: 2503\n",
      "epoch: 2249, loss: 16.582700729370117, rewards: -19.0, count: 1923\n",
      "epoch: 2259, loss: 31.881641387939453, rewards: -18.0, count: 2782\n",
      "epoch: 2269, loss: 35.74083709716797, rewards: -17.0, count: 2474\n",
      "epoch: 2279, loss: 38.22401809692383, rewards: -20.0, count: 1322\n",
      "epoch: 2289, loss: 25.107242584228516, rewards: -18.0, count: 3084\n",
      "epoch: 2299, loss: 18.618194580078125, rewards: -18.0, count: 2290\n",
      "epoch: 2309, loss: 20.724821090698242, rewards: -19.0, count: 2407\n",
      "epoch: 2319, loss: 27.5118408203125, rewards: -19.0, count: 1504\n",
      "epoch: 2329, loss: 17.1287899017334, rewards: -20.0, count: 1719\n",
      "epoch: 2339, loss: -11.443779945373535, rewards: -14.0, count: 2723\n",
      "epoch: 2349, loss: 40.055843353271484, rewards: -21.0, count: 2084\n",
      "epoch: 2359, loss: 38.34463882446289, rewards: -17.0, count: 2319\n",
      "epoch: 2369, loss: 22.13416862487793, rewards: -19.0, count: 2347\n",
      "epoch: 2379, loss: 2.112222194671631, rewards: -20.0, count: 2038\n",
      "epoch: 2389, loss: -2.3149819374084473, rewards: -19.0, count: 1919\n",
      "epoch: 2399, loss: 21.650043487548828, rewards: -18.0, count: 2378\n",
      "epoch: 2409, loss: -1.5988260507583618, rewards: -21.0, count: 1751\n",
      "epoch: 2419, loss: -1.6521377563476562, rewards: -17.0, count: 2987\n",
      "epoch: 2429, loss: 50.41761016845703, rewards: -20.0, count: 2195\n",
      "epoch: 2439, loss: 14.668992042541504, rewards: -20.0, count: 1801\n",
      "epoch: 2449, loss: 5.827144622802734, rewards: -17.0, count: 2014\n",
      "epoch: 2459, loss: 13.209238052368164, rewards: -19.0, count: 2009\n",
      "epoch: 2469, loss: 43.482627868652344, rewards: -19.0, count: 2095\n",
      "epoch: 2479, loss: 30.86241340637207, rewards: -21.0, count: 1671\n",
      "epoch: 2489, loss: -1.3335628509521484, rewards: -19.0, count: 2325\n",
      "epoch: 2499, loss: 13.229846954345703, rewards: -18.0, count: 2865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2509, loss: 36.82316589355469, rewards: -19.0, count: 2644\n",
      "epoch: 2519, loss: 18.072111129760742, rewards: -18.0, count: 2311\n",
      "epoch: 2529, loss: 9.523689270019531, rewards: -18.0, count: 1738\n",
      "epoch: 2539, loss: 33.95619201660156, rewards: -18.0, count: 2465\n",
      "epoch: 2549, loss: 15.05644416809082, rewards: -19.0, count: 2020\n",
      "epoch: 2559, loss: 27.503055572509766, rewards: -21.0, count: 1504\n",
      "epoch: 2569, loss: 23.564620971679688, rewards: -18.0, count: 1980\n",
      "epoch: 2579, loss: 2.8135719299316406, rewards: -19.0, count: 2089\n",
      "epoch: 2589, loss: 20.001726150512695, rewards: -20.0, count: 2196\n",
      "epoch: 2599, loss: 28.566858291625977, rewards: -19.0, count: 2481\n",
      "epoch: 2609, loss: 6.392823219299316, rewards: -19.0, count: 1913\n",
      "epoch: 2619, loss: 12.390830993652344, rewards: -19.0, count: 2017\n",
      "epoch: 2629, loss: 26.430553436279297, rewards: -16.0, count: 2548\n",
      "epoch: 2639, loss: 35.101131439208984, rewards: -14.0, count: 2569\n",
      "epoch: 2649, loss: -2.57442045211792, rewards: -19.0, count: 2165\n",
      "epoch: 2659, loss: 18.969987869262695, rewards: -20.0, count: 2275\n",
      "epoch: 2669, loss: 15.252248764038086, rewards: -19.0, count: 2250\n",
      "epoch: 2679, loss: 13.566554069519043, rewards: -18.0, count: 2219\n",
      "epoch: 2689, loss: 27.722970962524414, rewards: -19.0, count: 2000\n",
      "epoch: 2699, loss: 23.162817001342773, rewards: -20.0, count: 1458\n",
      "epoch: 2709, loss: 18.9691162109375, rewards: -16.0, count: 2618\n",
      "epoch: 2719, loss: 7.4364190101623535, rewards: -18.0, count: 2147\n",
      "epoch: 2729, loss: 5.038547515869141, rewards: -18.0, count: 2376\n",
      "epoch: 2739, loss: 2.103756904602051, rewards: -19.0, count: 2477\n",
      "epoch: 2749, loss: 9.071415901184082, rewards: -20.0, count: 2285\n",
      "epoch: 2759, loss: -0.020179390907287598, rewards: -19.0, count: 2333\n",
      "epoch: 2769, loss: 26.38640785217285, rewards: -18.0, count: 2538\n",
      "epoch: 2779, loss: 10.215925216674805, rewards: -17.0, count: 2341\n",
      "epoch: 2789, loss: -0.14096999168395996, rewards: -13.0, count: 3013\n",
      "epoch: 2799, loss: 29.13753890991211, rewards: -18.0, count: 2462\n",
      "epoch: 2809, loss: 25.843332290649414, rewards: -21.0, count: 1919\n",
      "epoch: 2819, loss: 41.82000732421875, rewards: -21.0, count: 1753\n",
      "epoch: 2829, loss: 14.676697731018066, rewards: -17.0, count: 2794\n",
      "epoch: 2839, loss: 32.15204620361328, rewards: -16.0, count: 2703\n",
      "epoch: 2849, loss: 14.04631233215332, rewards: -17.0, count: 2673\n",
      "epoch: 2859, loss: -7.209235191345215, rewards: -17.0, count: 2685\n",
      "epoch: 2869, loss: 29.97321891784668, rewards: -18.0, count: 2681\n",
      "epoch: 2879, loss: 8.229819297790527, rewards: -19.0, count: 2649\n",
      "epoch: 2889, loss: 21.11924934387207, rewards: -19.0, count: 2616\n",
      "epoch: 2899, loss: -4.041370391845703, rewards: -20.0, count: 2039\n",
      "epoch: 2909, loss: -3.4278006553649902, rewards: -19.0, count: 2817\n",
      "epoch: 2919, loss: 6.456151008605957, rewards: -19.0, count: 1919\n",
      "epoch: 2929, loss: -11.357070922851562, rewards: -20.0, count: 2120\n",
      "epoch: 2939, loss: 22.92833709716797, rewards: -11.0, count: 2951\n",
      "epoch: 2949, loss: 20.332550048828125, rewards: -17.0, count: 2260\n",
      "epoch: 2959, loss: 6.231314659118652, rewards: -20.0, count: 2124\n",
      "epoch: 2969, loss: 35.59444808959961, rewards: -18.0, count: 3507\n",
      "epoch: 2979, loss: 0.36409950256347656, rewards: -20.0, count: 2366\n",
      "epoch: 2989, loss: 26.054759979248047, rewards: -16.0, count: 2359\n",
      "epoch: 2999, loss: 0.7143096923828125, rewards: -17.0, count: 2089\n",
      "epoch: 3009, loss: 16.587820053100586, rewards: -18.0, count: 2612\n",
      "epoch: 3019, loss: 20.586383819580078, rewards: -17.0, count: 3160\n",
      "epoch: 3029, loss: 26.03333282470703, rewards: -16.0, count: 1803\n",
      "epoch: 3039, loss: 3.1342720985412598, rewards: -19.0, count: 2171\n",
      "epoch: 3049, loss: 10.634333610534668, rewards: -18.0, count: 2603\n",
      "epoch: 3059, loss: 28.612430572509766, rewards: -18.0, count: 3257\n",
      "epoch: 3069, loss: 26.983070373535156, rewards: -17.0, count: 2809\n",
      "epoch: 3079, loss: 22.887399673461914, rewards: -15.0, count: 3961\n",
      "epoch: 3089, loss: 10.311538696289062, rewards: -19.0, count: 2412\n",
      "epoch: 3099, loss: 32.07586669921875, rewards: -19.0, count: 3074\n",
      "epoch: 3109, loss: 10.974812507629395, rewards: -19.0, count: 2687\n",
      "epoch: 3119, loss: 19.702512741088867, rewards: -21.0, count: 1903\n",
      "epoch: 3129, loss: 27.04773712158203, rewards: -20.0, count: 3057\n",
      "epoch: 3139, loss: 7.404648303985596, rewards: -20.0, count: 2191\n",
      "epoch: 3149, loss: 12.048452377319336, rewards: -12.0, count: 2892\n",
      "epoch: 3159, loss: 18.799388885498047, rewards: -21.0, count: 2567\n",
      "epoch: 3169, loss: 12.381324768066406, rewards: -17.0, count: 3492\n",
      "epoch: 3179, loss: -0.5678082704544067, rewards: -18.0, count: 2513\n",
      "epoch: 3189, loss: 24.9942569732666, rewards: -17.0, count: 2912\n",
      "epoch: 3199, loss: 15.244224548339844, rewards: -17.0, count: 3317\n",
      "epoch: 3209, loss: 0.20455598831176758, rewards: -17.0, count: 2643\n",
      "epoch: 3219, loss: 60.32115173339844, rewards: -18.0, count: 2380\n",
      "epoch: 3229, loss: 5.335649013519287, rewards: -18.0, count: 2701\n",
      "epoch: 3239, loss: -16.220556259155273, rewards: -17.0, count: 3156\n",
      "epoch: 3249, loss: -10.27143383026123, rewards: -17.0, count: 1922\n",
      "epoch: 3259, loss: 31.608997344970703, rewards: -21.0, count: 2714\n",
      "epoch: 3269, loss: 6.902935981750488, rewards: -18.0, count: 1974\n",
      "epoch: 3279, loss: 10.04454517364502, rewards: -18.0, count: 1885\n",
      "epoch: 3289, loss: 26.96995735168457, rewards: -18.0, count: 2932\n",
      "epoch: 3299, loss: 4.980628967285156, rewards: -19.0, count: 2398\n",
      "epoch: 3309, loss: 25.60478401184082, rewards: -17.0, count: 2172\n",
      "epoch: 3319, loss: 22.04473114013672, rewards: -20.0, count: 3585\n",
      "epoch: 3329, loss: 8.732519149780273, rewards: -14.0, count: 3616\n",
      "epoch: 3339, loss: -3.5386786460876465, rewards: -17.0, count: 2927\n",
      "epoch: 3349, loss: 12.61469841003418, rewards: -17.0, count: 3214\n",
      "epoch: 3359, loss: -11.771888732910156, rewards: -18.0, count: 3345\n",
      "epoch: 3369, loss: 12.711446762084961, rewards: -19.0, count: 2839\n",
      "epoch: 3379, loss: 20.71311378479004, rewards: -16.0, count: 2282\n",
      "epoch: 3389, loss: 3.035541534423828, rewards: -20.0, count: 2053\n",
      "epoch: 3399, loss: 26.08403778076172, rewards: -20.0, count: 2272\n",
      "epoch: 3409, loss: 34.50166320800781, rewards: -18.0, count: 3349\n",
      "epoch: 3419, loss: 45.73535919189453, rewards: -20.0, count: 2698\n",
      "epoch: 3429, loss: -2.8699514865875244, rewards: -19.0, count: 2570\n",
      "epoch: 3439, loss: 22.96877670288086, rewards: -19.0, count: 2656\n",
      "epoch: 3449, loss: 26.175657272338867, rewards: -18.0, count: 2681\n",
      "epoch: 3459, loss: -8.975232124328613, rewards: -16.0, count: 3360\n",
      "epoch: 3469, loss: 10.749402046203613, rewards: -18.0, count: 3424\n",
      "epoch: 3479, loss: 11.36347770690918, rewards: -16.0, count: 3437\n",
      "epoch: 3489, loss: 33.76482009887695, rewards: -20.0, count: 2738\n",
      "epoch: 3499, loss: 13.471776008605957, rewards: -18.0, count: 2930\n",
      "epoch: 3509, loss: 2.9630961418151855, rewards: -15.0, count: 2836\n",
      "epoch: 3519, loss: 25.441316604614258, rewards: -16.0, count: 2770\n",
      "epoch: 3529, loss: -29.175559997558594, rewards: -15.0, count: 3244\n",
      "epoch: 3539, loss: 13.460638046264648, rewards: -17.0, count: 2494\n",
      "epoch: 3549, loss: 3.0085458755493164, rewards: -18.0, count: 2617\n",
      "epoch: 3559, loss: 11.243640899658203, rewards: -20.0, count: 2201\n",
      "epoch: 3569, loss: 9.085655212402344, rewards: -18.0, count: 3176\n",
      "epoch: 3579, loss: 18.97519302368164, rewards: -19.0, count: 2645\n",
      "epoch: 3589, loss: 4.971386909484863, rewards: -18.0, count: 2710\n",
      "epoch: 3599, loss: 22.276094436645508, rewards: -20.0, count: 2503\n",
      "epoch: 3609, loss: -10.928298950195312, rewards: -19.0, count: 3384\n",
      "epoch: 3619, loss: -10.070712089538574, rewards: -18.0, count: 2780\n",
      "epoch: 3629, loss: 26.178651809692383, rewards: -20.0, count: 2293\n",
      "epoch: 3639, loss: 12.808349609375, rewards: -19.0, count: 3076\n",
      "epoch: 3649, loss: -10.749175071716309, rewards: -18.0, count: 2937\n",
      "epoch: 3659, loss: 15.454005241394043, rewards: -20.0, count: 1962\n",
      "epoch: 3669, loss: 14.883060455322266, rewards: -20.0, count: 2029\n",
      "epoch: 3679, loss: -6.894343376159668, rewards: -18.0, count: 2306\n",
      "epoch: 3689, loss: 34.464630126953125, rewards: -17.0, count: 2831\n",
      "epoch: 3699, loss: 34.735103607177734, rewards: -19.0, count: 3303\n",
      "epoch: 3709, loss: 13.020025253295898, rewards: -18.0, count: 2921\n",
      "epoch: 3719, loss: 3.5461249351501465, rewards: -19.0, count: 3134\n",
      "epoch: 3729, loss: 4.537604331970215, rewards: -19.0, count: 2558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3739, loss: 34.86646270751953, rewards: -20.0, count: 2910\n",
      "epoch: 3749, loss: 40.12380599975586, rewards: -18.0, count: 2600\n",
      "epoch: 3759, loss: 1.619724154472351, rewards: -19.0, count: 3876\n",
      "epoch: 3769, loss: 30.28267478942871, rewards: -19.0, count: 2964\n",
      "epoch: 3779, loss: -36.03532791137695, rewards: -17.0, count: 4134\n",
      "epoch: 3789, loss: 2.9764137268066406, rewards: -18.0, count: 3689\n",
      "epoch: 3799, loss: 23.281028747558594, rewards: -19.0, count: 2324\n",
      "epoch: 3809, loss: 50.944889068603516, rewards: -17.0, count: 4035\n",
      "epoch: 3819, loss: 5.531700134277344, rewards: -14.0, count: 3927\n",
      "epoch: 3829, loss: 23.434860229492188, rewards: -20.0, count: 2863\n",
      "epoch: 3839, loss: 5.27295446395874, rewards: -18.0, count: 2842\n",
      "epoch: 3849, loss: -0.329531192779541, rewards: -19.0, count: 3053\n",
      "epoch: 3859, loss: 18.55010223388672, rewards: -20.0, count: 1798\n",
      "epoch: 3869, loss: 17.076791763305664, rewards: -20.0, count: 2867\n",
      "epoch: 3879, loss: 14.21294116973877, rewards: -17.0, count: 2798\n",
      "epoch: 3889, loss: -7.846541404724121, rewards: -16.0, count: 2796\n",
      "epoch: 3899, loss: 13.848626136779785, rewards: -14.0, count: 3827\n",
      "epoch: 3909, loss: 15.875163078308105, rewards: -14.0, count: 3494\n",
      "epoch: 3919, loss: 13.475634574890137, rewards: -16.0, count: 2949\n",
      "epoch: 3929, loss: -0.08424615859985352, rewards: -17.0, count: 2183\n",
      "epoch: 3939, loss: 23.470571517944336, rewards: -16.0, count: 3032\n",
      "epoch: 3949, loss: 27.254995346069336, rewards: -19.0, count: 2644\n",
      "epoch: 3959, loss: 9.504781723022461, rewards: -16.0, count: 2926\n",
      "epoch: 3969, loss: 27.420751571655273, rewards: -13.0, count: 2990\n",
      "epoch: 3979, loss: -16.286176681518555, rewards: -17.0, count: 2814\n",
      "epoch: 3989, loss: 20.266220092773438, rewards: -20.0, count: 3073\n",
      "epoch: 3999, loss: 19.8577938079834, rewards: -20.0, count: 1563\n",
      "epoch: 4009, loss: -0.22520828247070312, rewards: -17.0, count: 3058\n",
      "epoch: 4019, loss: 5.986835479736328, rewards: -18.0, count: 2297\n",
      "epoch: 4029, loss: 18.582012176513672, rewards: -15.0, count: 3542\n",
      "epoch: 4039, loss: 14.085817337036133, rewards: -20.0, count: 2784\n",
      "epoch: 4049, loss: 2.9380154609680176, rewards: -19.0, count: 2216\n",
      "epoch: 4059, loss: 2.8081417083740234, rewards: -17.0, count: 2395\n",
      "epoch: 4069, loss: 13.840435028076172, rewards: -17.0, count: 2744\n",
      "epoch: 4079, loss: 23.895471572875977, rewards: -14.0, count: 3285\n",
      "epoch: 4089, loss: 37.077430725097656, rewards: -16.0, count: 3594\n",
      "epoch: 4099, loss: -11.799736976623535, rewards: -15.0, count: 3313\n",
      "epoch: 4109, loss: -10.508411407470703, rewards: -13.0, count: 3006\n",
      "epoch: 4119, loss: 7.064023971557617, rewards: -19.0, count: 2616\n",
      "epoch: 4129, loss: 9.408308029174805, rewards: -20.0, count: 2931\n",
      "epoch: 4139, loss: 8.530838966369629, rewards: -18.0, count: 2926\n",
      "epoch: 4149, loss: 19.537036895751953, rewards: -17.0, count: 2975\n",
      "epoch: 4159, loss: -11.184276580810547, rewards: -14.0, count: 3325\n",
      "epoch: 4169, loss: 28.955955505371094, rewards: -20.0, count: 2519\n",
      "epoch: 4179, loss: 25.130979537963867, rewards: -16.0, count: 2718\n",
      "epoch: 4189, loss: 20.634986877441406, rewards: -16.0, count: 3294\n",
      "epoch: 4199, loss: 4.139565467834473, rewards: -20.0, count: 2518\n",
      "epoch: 4209, loss: 25.378814697265625, rewards: -19.0, count: 3305\n",
      "epoch: 4219, loss: 6.865243911743164, rewards: -18.0, count: 3189\n",
      "epoch: 4229, loss: -0.3793487548828125, rewards: -18.0, count: 2925\n",
      "epoch: 4239, loss: 46.8665771484375, rewards: -19.0, count: 2946\n",
      "epoch: 4249, loss: 4.501293659210205, rewards: -14.0, count: 2878\n",
      "epoch: 4259, loss: 26.81340789794922, rewards: -16.0, count: 3406\n",
      "epoch: 4269, loss: 20.569049835205078, rewards: -18.0, count: 2543\n",
      "epoch: 4279, loss: 32.79151153564453, rewards: -13.0, count: 3402\n",
      "epoch: 4289, loss: 45.85411834716797, rewards: -14.0, count: 3954\n",
      "epoch: 4299, loss: 12.642748832702637, rewards: -18.0, count: 2526\n",
      "epoch: 4309, loss: 12.908225059509277, rewards: -18.0, count: 3168\n",
      "epoch: 4319, loss: 16.738162994384766, rewards: -16.0, count: 3344\n",
      "epoch: 4329, loss: 28.879222869873047, rewards: -17.0, count: 3457\n",
      "epoch: 4339, loss: 4.741826057434082, rewards: -18.0, count: 3853\n",
      "epoch: 4349, loss: -5.611111640930176, rewards: -17.0, count: 3875\n",
      "epoch: 4359, loss: 9.79575252532959, rewards: -16.0, count: 3127\n",
      "epoch: 4369, loss: 27.539749145507812, rewards: -15.0, count: 3847\n",
      "epoch: 4379, loss: 12.073147773742676, rewards: -19.0, count: 2960\n",
      "epoch: 4389, loss: -12.116470336914062, rewards: -20.0, count: 3172\n",
      "epoch: 4399, loss: -7.644408226013184, rewards: -16.0, count: 3031\n",
      "epoch: 4409, loss: -11.937139511108398, rewards: -17.0, count: 3952\n",
      "epoch: 4419, loss: -13.498495101928711, rewards: -11.0, count: 3570\n",
      "epoch: 4429, loss: -4.086561679840088, rewards: -18.0, count: 2789\n",
      "epoch: 4439, loss: -4.2564802169799805, rewards: -16.0, count: 3183\n",
      "epoch: 4449, loss: 7.912384510040283, rewards: -14.0, count: 3763\n",
      "epoch: 4459, loss: -15.281097412109375, rewards: -14.0, count: 3272\n",
      "epoch: 4469, loss: 10.077451705932617, rewards: -16.0, count: 4080\n",
      "epoch: 4479, loss: 13.093477249145508, rewards: -18.0, count: 3256\n",
      "epoch: 4489, loss: 5.319223880767822, rewards: -18.0, count: 3250\n",
      "epoch: 4499, loss: 21.05575942993164, rewards: -19.0, count: 3210\n",
      "epoch: 4509, loss: -5.91256856918335, rewards: -13.0, count: 4290\n",
      "epoch: 4519, loss: 43.86187744140625, rewards: -20.0, count: 3020\n",
      "epoch: 4529, loss: -14.785266876220703, rewards: -20.0, count: 2012\n",
      "epoch: 4539, loss: 3.512451171875, rewards: -14.0, count: 3687\n",
      "epoch: 4549, loss: -17.99301528930664, rewards: -16.0, count: 3394\n",
      "epoch: 4559, loss: 4.745067596435547, rewards: -15.0, count: 3804\n",
      "epoch: 4569, loss: 18.071380615234375, rewards: -21.0, count: 2651\n",
      "epoch: 4579, loss: 1.451692819595337, rewards: -14.0, count: 3764\n",
      "epoch: 4589, loss: -8.780564308166504, rewards: -17.0, count: 2586\n",
      "epoch: 4599, loss: -6.684015274047852, rewards: -18.0, count: 2299\n",
      "epoch: 4609, loss: -4.967051029205322, rewards: -12.0, count: 3916\n",
      "epoch: 4619, loss: 25.4490966796875, rewards: -18.0, count: 3497\n",
      "epoch: 4629, loss: 12.603137969970703, rewards: -18.0, count: 3441\n",
      "epoch: 4639, loss: 25.243173599243164, rewards: -10.0, count: 3945\n",
      "epoch: 4649, loss: -3.888558864593506, rewards: -18.0, count: 3350\n",
      "epoch: 4659, loss: 8.10706901550293, rewards: -17.0, count: 3235\n",
      "epoch: 4669, loss: -30.913108825683594, rewards: -16.0, count: 4455\n",
      "epoch: 4679, loss: -5.438608169555664, rewards: -18.0, count: 3080\n",
      "epoch: 4689, loss: 10.946178436279297, rewards: -17.0, count: 3559\n",
      "epoch: 4699, loss: -8.337201118469238, rewards: -16.0, count: 3599\n",
      "epoch: 4709, loss: 21.211320877075195, rewards: -20.0, count: 2683\n",
      "epoch: 4719, loss: 46.539371490478516, rewards: -20.0, count: 3248\n",
      "epoch: 4729, loss: 2.5402255058288574, rewards: -19.0, count: 3140\n",
      "epoch: 4739, loss: 13.723662376403809, rewards: -18.0, count: 2954\n",
      "epoch: 4749, loss: 18.162429809570312, rewards: -16.0, count: 4342\n",
      "epoch: 4759, loss: 8.27946949005127, rewards: -14.0, count: 3649\n",
      "epoch: 4769, loss: 24.974292755126953, rewards: -9.0, count: 5520\n",
      "epoch: 4779, loss: -29.238040924072266, rewards: -14.0, count: 3520\n",
      "epoch: 4789, loss: -14.605192184448242, rewards: -20.0, count: 2688\n",
      "epoch: 4799, loss: -12.868788719177246, rewards: -17.0, count: 3056\n",
      "epoch: 4809, loss: -2.4931764602661133, rewards: -18.0, count: 3092\n",
      "epoch: 4819, loss: 8.562844276428223, rewards: -18.0, count: 2783\n",
      "epoch: 4829, loss: -20.45607566833496, rewards: -14.0, count: 3042\n",
      "epoch: 4839, loss: 23.25880241394043, rewards: -16.0, count: 3766\n",
      "epoch: 4849, loss: 35.839500427246094, rewards: -18.0, count: 2866\n",
      "epoch: 4859, loss: 0.9419727325439453, rewards: -14.0, count: 3931\n",
      "epoch: 4869, loss: 31.342327117919922, rewards: -18.0, count: 2615\n",
      "epoch: 4879, loss: 33.63850402832031, rewards: -15.0, count: 4378\n",
      "epoch: 4889, loss: 0.1569514274597168, rewards: -9.0, count: 4099\n",
      "epoch: 4899, loss: 5.014046669006348, rewards: -14.0, count: 3034\n",
      "epoch: 4909, loss: -3.8742713928222656, rewards: -14.0, count: 4486\n",
      "epoch: 4919, loss: -12.876766204833984, rewards: -14.0, count: 3271\n",
      "epoch: 4929, loss: 6.640364646911621, rewards: -14.0, count: 3443\n",
      "epoch: 4939, loss: 3.416198968887329, rewards: -13.0, count: 3673\n",
      "epoch: 4949, loss: -15.962251663208008, rewards: -8.0, count: 5008\n",
      "epoch: 4959, loss: -20.499610900878906, rewards: -16.0, count: 3108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4969, loss: -46.909061431884766, rewards: -17.0, count: 4125\n",
      "epoch: 4979, loss: 1.9770617485046387, rewards: -18.0, count: 2208\n",
      "epoch: 4989, loss: -19.459033966064453, rewards: -16.0, count: 3443\n",
      "epoch: 4999, loss: -24.501022338867188, rewards: -15.0, count: 3138\n",
      "epoch: 5009, loss: -2.1939568519592285, rewards: -14.0, count: 3923\n",
      "epoch: 5019, loss: 0.3761183023452759, rewards: -20.0, count: 2500\n",
      "epoch: 5029, loss: 3.325768232345581, rewards: -7.0, count: 5052\n",
      "epoch: 5039, loss: -27.791160583496094, rewards: -13.0, count: 5128\n",
      "epoch: 5049, loss: 35.28559112548828, rewards: -16.0, count: 3453\n",
      "epoch: 5059, loss: -12.46896743774414, rewards: -9.0, count: 4402\n",
      "epoch: 5069, loss: -0.4447190761566162, rewards: -16.0, count: 3099\n",
      "epoch: 5079, loss: -10.230735778808594, rewards: -17.0, count: 3703\n",
      "epoch: 5089, loss: 6.249671936035156, rewards: -16.0, count: 3670\n",
      "epoch: 5099, loss: 4.142050266265869, rewards: -15.0, count: 3612\n",
      "epoch: 5109, loss: -0.892456591129303, rewards: -18.0, count: 3401\n",
      "epoch: 5119, loss: -12.385408401489258, rewards: -12.0, count: 4007\n",
      "epoch: 5129, loss: 10.570292472839355, rewards: -20.0, count: 1704\n",
      "epoch: 5139, loss: -21.166671752929688, rewards: -12.0, count: 3757\n",
      "epoch: 5149, loss: 16.320648193359375, rewards: -13.0, count: 4144\n",
      "epoch: 5159, loss: 24.87763023376465, rewards: -16.0, count: 3133\n",
      "epoch: 5169, loss: -9.689745903015137, rewards: -7.0, count: 3843\n",
      "epoch: 5179, loss: -0.5353116989135742, rewards: -17.0, count: 3902\n",
      "epoch: 5189, loss: 9.853097915649414, rewards: -15.0, count: 2593\n",
      "epoch: 5199, loss: 13.523155212402344, rewards: -11.0, count: 4222\n",
      "epoch: 5209, loss: 3.0084636211395264, rewards: -14.0, count: 3140\n",
      "epoch: 5219, loss: 10.67807674407959, rewards: -18.0, count: 3185\n",
      "epoch: 5229, loss: 5.703308582305908, rewards: -20.0, count: 1559\n",
      "epoch: 5239, loss: -3.207050323486328, rewards: -19.0, count: 4153\n",
      "epoch: 5249, loss: -10.284156799316406, rewards: -18.0, count: 3656\n",
      "epoch: 5259, loss: 20.433696746826172, rewards: -11.0, count: 4538\n",
      "epoch: 5269, loss: -2.5451042652130127, rewards: -14.0, count: 3220\n",
      "epoch: 5279, loss: 6.334512710571289, rewards: -20.0, count: 3264\n",
      "epoch: 5289, loss: 38.13569259643555, rewards: -19.0, count: 3626\n",
      "epoch: 5299, loss: -9.540210723876953, rewards: -9.0, count: 4738\n",
      "epoch: 5309, loss: -10.532771110534668, rewards: -16.0, count: 3360\n",
      "epoch: 5319, loss: 27.382991790771484, rewards: -18.0, count: 3340\n",
      "epoch: 5329, loss: 1.2111549377441406, rewards: -17.0, count: 3783\n",
      "epoch: 5339, loss: -3.407925844192505, rewards: -16.0, count: 2859\n",
      "epoch: 5349, loss: -19.742534637451172, rewards: -14.0, count: 4313\n",
      "epoch: 5359, loss: 9.75800895690918, rewards: -20.0, count: 2555\n",
      "epoch: 5369, loss: -4.043458461761475, rewards: -19.0, count: 2622\n",
      "epoch: 5379, loss: -3.3556718826293945, rewards: -19.0, count: 2803\n",
      "epoch: 5389, loss: -15.210001945495605, rewards: -14.0, count: 3119\n",
      "epoch: 5399, loss: 25.515972137451172, rewards: -12.0, count: 4289\n",
      "epoch: 5409, loss: -19.178119659423828, rewards: -16.0, count: 3531\n",
      "epoch: 5419, loss: 1.2063286304473877, rewards: -16.0, count: 3119\n",
      "epoch: 5429, loss: -3.1337766647338867, rewards: -14.0, count: 3514\n",
      "epoch: 5439, loss: -17.13393783569336, rewards: -16.0, count: 3769\n",
      "epoch: 5449, loss: 10.843820571899414, rewards: -11.0, count: 4402\n",
      "epoch: 5459, loss: 20.311262130737305, rewards: -11.0, count: 4978\n",
      "epoch: 5469, loss: -6.064872741699219, rewards: -17.0, count: 3475\n",
      "epoch: 5479, loss: -11.338905334472656, rewards: -15.0, count: 2914\n",
      "epoch: 5489, loss: 11.138409614562988, rewards: -12.0, count: 4295\n",
      "epoch: 5499, loss: -1.000077724456787, rewards: -18.0, count: 4051\n",
      "epoch: 5509, loss: 15.69914722442627, rewards: -18.0, count: 3335\n",
      "epoch: 5519, loss: -16.465896606445312, rewards: -11.0, count: 5347\n",
      "epoch: 5529, loss: -33.145782470703125, rewards: -12.0, count: 4783\n",
      "epoch: 5539, loss: 4.788791179656982, rewards: -18.0, count: 3916\n",
      "epoch: 5549, loss: 1.620715618133545, rewards: -18.0, count: 3670\n",
      "epoch: 5559, loss: -6.314736843109131, rewards: -14.0, count: 3575\n",
      "epoch: 5569, loss: -20.017932891845703, rewards: -15.0, count: 5183\n",
      "epoch: 5579, loss: 13.596969604492188, rewards: -3.0, count: 5232\n",
      "epoch: 5589, loss: 3.225123405456543, rewards: -15.0, count: 4790\n",
      "epoch: 5599, loss: -27.216943740844727, rewards: -13.0, count: 4196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPong-v4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[97], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(agent, env)\u001b[0m\n\u001b[1;32m     18\u001b[0m             rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     19\u001b[0m             log_probs\u001b[38;5;241m.\u001b[39mappend(log_prob)\n\u001b[0;32m---> 21\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#             torch.save('pong.pt',agent.policy_net)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, rewards: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[79], line 38\u001b[0m, in \u001b[0;36mAgent.update\u001b[0;34m(self, rewards, log_probs)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m r_log_probs\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mem0/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v4\")\n",
    "train(agent,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b5a24b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(self,state):\n",
    "    probs = self.policy_net(state) # 4\n",
    "    if np.random.uniform() < 0.0:\n",
    "        action = np.random.randint(0,2)\n",
    "        return action + 2, torch.log(probs[action]+1e-8)\n",
    "    dist = Categorical(probs)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action.item()+2,log_prob\n",
    "\n",
    "# 替换方法\n",
    "import types\n",
    "agent.sample_action = types.MethodType(sample_action, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ac343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def visualize_agent(env, agent, num_episodes=5):\n",
    "    \"\"\"\n",
    "    渲染显示智能体的行动\n",
    "    \"\"\"\n",
    "    env = gym.make('CliffWalking-v0', render_mode='human')  # 创建可视化环境\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state_tuple = env.reset()\n",
    "        state = state_tuple[0] if isinstance(state_tuple, tuple) else state_tuple\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}\")\n",
    "        \n",
    "        while not done:\n",
    "            env.render()  # 渲染当前状态\n",
    "            \n",
    "            # 将状态转换为one-hot编码\n",
    "            state_onehot = np.zeros(48)\n",
    "            state_onehot[state] = 1\n",
    "            \n",
    "            # 使用训练好的策略选择动作\n",
    "            with torch.no_grad():\n",
    "                if np.random.random() < 0.0:\n",
    "                    action = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    state_tensor = torch.FloatTensor(state_onehot)\n",
    "                    probs = agent.policy_net(state_tensor)\n",
    "                    action = probs.argmax().item()  # 使用最可能的动作\n",
    "            \n",
    "            # 执行动作\n",
    "            step_result = env.step(action)\n",
    "            if len(step_result) == 4:\n",
    "                next_state, reward, done, _ = step_result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, _ = step_result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            # 添加小延迟使动作更容易观察\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        print(f\"Episode finished after {steps} steps. Total reward: {total_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# 在主程序最后添加：\n",
    "if __name__ == \"__main__\":    \n",
    "    # 训练完成后显示智能体行动\n",
    "    print(\"\\nVisualizing trained agent behavior...\")\n",
    "    env = gym.make('CliffWalking-v0',render_mode='human')\n",
    "    visualize_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1049fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
