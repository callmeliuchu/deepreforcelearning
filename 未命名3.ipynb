{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27f2998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "272b8703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a357d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c7ff553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "np.bool8 = np.bool_\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7699f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")  # 或 \"rgb_array\"\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "state,_ = env.reset()\n",
    "next_state, reward, terminated, truncated, _ = env.step(0)  # 注意这里的返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae24706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,ouput_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim,ouput_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        probs = F.softmax(x,dim=-1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e4cd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,ouput_dim,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim,ouput_dim)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ceeef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    " class ReplayQue:\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.buffer = deque()\n",
    "        \n",
    "        \n",
    "        def push(self,transitions):\n",
    "            self.buffer.append(transitions)\n",
    "        \n",
    "        def clear(self):\n",
    "            self.buffer.clear()\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.buffer)\n",
    "        \n",
    "        def sample(self):\n",
    "            return zip(*list(self.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3feb935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.actor = Actor(n_states,n_actions,256)\n",
    "        self.critic = Critic(n_states,1,256)\n",
    "        self.actor_optimizer = Adam(self.actor.parameters())\n",
    "        self.critic_optimizer = Adam(self.critic.parameters())\n",
    "        self.memory = ReplayQue()\n",
    "    \n",
    "    \n",
    "    def sample_action(self,state):\n",
    "        state = torch.tensor(state)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.detach().cpu().numpy().item(), log_prob.detach()\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self,state):\n",
    "        state = torch.tensor(state)\n",
    "        probs = self.actor(state)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.detach().cpu().numpy().item()\n",
    "        \n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.memory) % 100 != 0:\n",
    "            return\n",
    "    \n",
    "        \n",
    "        old_actions,old_states,old_rewards,old_dones,old_logprobs = self.memory.sample()\n",
    "\n",
    "        old_actions = torch.tensor(old_actions)\n",
    "        old_states = torch.tensor(old_states)\n",
    "        old_logprobs = torch.tensor(old_logprobs)\n",
    "        \n",
    "        returns = []\n",
    "        discount_sum = 0\n",
    "        for reward,done in zip(reversed(old_rewards),reversed(old_dones)):\n",
    "            if done:\n",
    "                discount_sum = 0\n",
    "            discount_sum = discount_sum * 0.99 + reward\n",
    "            returns.insert(0,discount_sum)\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "        \n",
    "        for _ in range(4):\n",
    "            values = self.critic(old_states)\n",
    "            probs = self.actor(old_states)\n",
    "            dist = Categorical(probs)\n",
    "            logprobs = dist.log_prob(old_actions)\n",
    "            ratio = torch.exp(logprobs - old_logprobs)\n",
    "            advantage = returns - values.detach()\n",
    "            surr1 = advantage * ratio\n",
    "            surr2 = torch.clamp(ratio,0.8,1.2) * advantage\n",
    "            actor_loss = -torch.min(surr1,surr2).mean() + 0.01 * dist.entropy().mean()\n",
    "            critic_loss = (returns - values).pow(2).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            \n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            \n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "        self.memory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb2f1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,agent):\n",
    "    \n",
    "    for eph in range(10000):\n",
    "        \n",
    "        ### train\n",
    "        state,_ = env.reset()\n",
    "        for _ in range(1000):\n",
    "            \n",
    "            action,logprob = agent.sample_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)  # 注意这里的返回值\n",
    "            agent.memory.push((action,state,reward,terminated,logprob))\n",
    "            state = next_state\n",
    "            agent.update()\n",
    "            if terminated:\n",
    "                break\n",
    "        \n",
    "        ### eval\n",
    "        if (eph+1) % 100 == 0:\n",
    "            reward_sum = 0\n",
    "            state,_ = env.reset()\n",
    "            for _ in range(1000):\n",
    "                action = agent.predict(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)  # 注意这里的返回值\n",
    "                reward_sum += reward\n",
    "                state = next_state\n",
    "                if terminated:\n",
    "                    break\n",
    "            print('reward....',reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d537f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7341659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward.... 44.0\n",
      "reward.... 247.0\n",
      "reward.... 258.0\n",
      "reward.... 174.0\n",
      "reward.... 95.0\n",
      "reward.... 287.0\n",
      "reward.... 113.0\n",
      "reward.... 208.0\n",
      "reward.... 258.0\n",
      "reward.... 201.0\n",
      "reward.... 172.0\n",
      "reward.... 200.0\n",
      "reward.... 144.0\n",
      "reward.... 223.0\n",
      "reward.... 167.0\n",
      "reward.... 105.0\n",
      "reward.... 97.0\n",
      "reward.... 78.0\n",
      "reward.... 183.0\n",
      "reward.... 148.0\n",
      "reward.... 439.0\n",
      "reward.... 148.0\n",
      "reward.... 159.0\n",
      "reward.... 505.0\n",
      "reward.... 1000.0\n",
      "reward.... 171.0\n",
      "reward.... 189.0\n",
      "reward.... 413.0\n",
      "reward.... 151.0\n",
      "reward.... 287.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 503.0\n",
      "reward.... 460.0\n",
      "reward.... 1000.0\n",
      "reward.... 884.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 395.0\n",
      "reward.... 575.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 502.0\n",
      "reward.... 443.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n",
      "reward.... 1000.0\n"
     ]
    }
   ],
   "source": [
    "train(env,agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d37ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82291f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdecf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830a3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mem0",
   "language": "python",
   "name": "mem0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
